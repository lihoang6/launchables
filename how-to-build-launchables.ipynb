{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ How to Build Brev Launchables\n",
        "\n",
        "## TL;DR - Why This Matters\n",
        "\n",
        "**The problem:** You built something awesome in AI. You share it. 98% of developers never see it work because they hit setup/GPU issues.\n",
        "\n",
        "**The solution:** Turn your notebook into a Launchable. Developers click a link, land in a GPU notebook in 30 seconds, and **actually try your work**.\n",
        "\n",
        "**The impact:** 30x more adoption, more GitHub stars, more visibility.\n",
        "\n",
        "**Time investment:** ~30 minutes to convert your existing notebook.\n",
        "\n",
        "---\n",
        "\n",
        "## What You'll Learn Today\n",
        "\n",
        "This tutorial teaches you how to transform your AI notebooks into **shareable, GPU-backed experiences** that anyone can run with one click.\n",
        "\n",
        "**You'll build:**\n",
        "- ‚úÖ GPU-verified interactive demos\n",
        "- ‚úÖ Shareable prototypes that eliminate setup friction\n",
        "- ‚úÖ Working examples that get TRIED, not just bookmarked\n",
        "- ‚úÖ Live showcases with automatic GPU access\n",
        "\n",
        "## Who This Is For\n",
        "\n",
        "**Built something in AI?** This is for you if you want to:\n",
        "- Share your work with the community\n",
        "- Make your innovations discoverable\n",
        "- Help others build faster\n",
        "\n",
        "## Tutorial Structure\n",
        "\n",
        "1. **Why Build Launchables?** - Value proposition and use cases\n",
        "2. **Your First Simple Demo** - Load and run a model on GPU\n",
        "3. **Making It Interactive** - Add engagement and user controls\n",
        "4. **Best Practices** - Patterns for great launchables\n",
        "5. **Sharing Your Launchable** - GitHub and Brev deployment\n",
        "6. **Your Challenge** - Build your own!\n",
        "\n",
        "**Estimated time:** 45-60 minutes\n",
        "\n",
        "---\n",
        "\n",
        "Let's get started! üëá\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Required Packages\n",
        "# This installs everything needed for this tutorial in your notebook's kernel\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install -q torch transformers accelerate matplotlib numpy tqdm ipywidgets\n",
        "\n",
        "print(\"=\"  * 60)\n",
        "print(\"‚úÖ Installation complete!\")\n",
        "print(\"=\"  * 60)\n",
        "print(\"\\n‚ö†Ô∏è  IMPORTANT: Please restart the kernel now\")\n",
        "print(\"   - Click 'Kernel' ‚Üí 'Restart' in the menu\")\n",
        "print(\"   - Then run all cells below\")\n",
        "print(\"=\"  * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Restart Kernel Required\n",
        "\n",
        "**After running the installation cell above:**\n",
        "1. Click `Kernel` ‚Üí `Restart` in the menu\n",
        "2. Run all cells below from this point forward\n",
        "\n",
        "This ensures the newly installed packages are loaded correctly.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: GPU Verification\n",
        "# This verifies your GPU is available and ready to use\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"üîç GPU Verification Report\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ CUDA Available: Yes\")\n",
        "    print(f\"‚úÖ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"‚úÖ CUDA Version: {torch.version.cuda}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"\\nüéâ GPU is ready!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No GPU detected\")\n",
        "    print(\"This tutorial requires a GPU-enabled instance.\")\n",
        "    print(\"Please deploy this launchable on a GPU-enabled environment.\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"\\nüéØ Using device: {device}\")\n",
        "print(\"=\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 1: Why Build Launchables? üéØ\n",
        "\n",
        "## The Problem: Your Innovation Dies in Setup Hell\n",
        "\n",
        "You've built something amazing - a faster training method, an optimized model, a breakthrough technique. You share it on GitHub with a README and demo notebook.\n",
        "\n",
        "**Here's what happens:**\n",
        "\n",
        "```\n",
        "100 developers see your post\n",
        "  ‚Üì\n",
        "  95 click your repo\n",
        "    ‚Üì\n",
        "    50 start reading README\n",
        "      ‚Üì\n",
        "      20 attempt \"pip install\"\n",
        "        ‚Üì\n",
        "        5 actually get it working\n",
        "          ‚Üì\n",
        "          2 try your demo\n",
        "```\n",
        "\n",
        "**Your innovation:** Buried in setup friction.  \n",
        "**Your impact:** 2% of interested developers.\n",
        "\n",
        "## The Solution: Launchables = Zero-Friction Demos\n",
        "\n",
        "A **Launchable** is a self-contained, GPU-backed notebook that runs with one click.\n",
        "\n",
        "**Same scenario with a launchable:**\n",
        "\n",
        "```\n",
        "100 developers see your post\n",
        "  ‚Üì\n",
        "  95 click \"Launch\"\n",
        "    ‚Üì\n",
        "    90 land in GPU notebook (30 seconds)\n",
        "      ‚Üì\n",
        "      85 run your demo successfully\n",
        "        ‚Üì\n",
        "        60 see your innovation work\n",
        "          ‚Üì\n",
        "          20 star your repo, 10 tweet about it\n",
        "```\n",
        "\n",
        "**Your innovation:** Actually seen and experienced.  \n",
        "**Your impact:** 60% of interested developers.  \n",
        "**Result:** 30x more people trying your work.\n",
        "\n",
        "## What Should You Turn Into a Launchable?\n",
        "\n",
        "**Perfect for:**\n",
        "- \"2x faster fine-tuning\" ‚Üí Run benchmark in 60 seconds\n",
        "- \"New architecture\" ‚Üí Side-by-side comparison with baseline\n",
        "- \"Optimization tool\" ‚Üí Before/after demo with real metrics\n",
        "- \"Paper reproduction\" ‚Üí One-click run of your results\n",
        "- \"Library showcase\" ‚Üí Interactive tutorial with examples\n",
        "\n",
        "**Real examples:**\n",
        "- Unsloth: Shows QLoRA speedup vs. Hugging Face (users see 2x instantly)\n",
        "- Axolotl: Compare training configs side-by-side (no setup required)\n",
        "- Your tool: Gets tried instead of bookmarked and forgotten\n",
        "\n",
        "## What's In It For You?\n",
        "\n",
        "**As a developer/company, launchables give you:**\n",
        "\n",
        "‚úÖ **More adoption:** 30x more devs actually TRY your work  \n",
        "‚úÖ **More visibility:** Featured in Brev showcase, shared on social  \n",
        "‚úÖ **More credibility:** \"It actually works\" > \"Trust me bro\"  \n",
        "‚úÖ **More feedback:** Users engage when friction is zero  \n",
        "‚úÖ **More stars:** Working demos = GitHub stars + tweets\n",
        "\n",
        "**Bottom line:** Your innovation gets USED, not just seen.\n",
        "\n",
        "---\n",
        "\n",
        "**In this section, you learned:** Why launchables 30x your reach and impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 2: Your First Simple Demo üöÄ\n",
        "\n",
        "Let's build a minimal working example. You'll:\n",
        "- Load a tiny model (DistilGPT2)\n",
        "- Place it explicitly on GPU\n",
        "- Run simple inference\n",
        "- Show GPU memory usage\n",
        "\n",
        "**This is the foundation of every launchable.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and run a simple model on GPU\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üöÄ Simple GPU Demo: Text Generation\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nüéØ This demo optimized for: 1x NVIDIA L40s or A100 40GB\")\n",
        "print(\"üí° Works on: Any NVIDIA GPU with 8GB+ VRAM\")\n",
        "\n",
        "# Load model (suppressing verbose output)\n",
        "print(\"\\nüì• Loading DistilGPT2...\")\n",
        "model_name = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU explicitly\n",
        "print(f\"üîÑ Moving model to {device}...\")\n",
        "model = model.to(device)\n",
        "_ = model.eval()\n",
        "\n",
        "# Verify placement\n",
        "model_device = next(model.parameters()).device\n",
        "print(f\"‚úÖ Model is on: {model_device}\")\n",
        "\n",
        "# Show GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    memory_used = torch.cuda.memory_allocated(0) / 1e9\n",
        "    print(f\"üíæ GPU Memory Used: {memory_used:.2f} GB\")\n",
        "\n",
        "# Generate text\n",
        "print(\"\\nüé® Generating text...\")\n",
        "prompt = \"Brev launchables make AI development\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=40,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "generation_time = time.time() - start\n",
        "\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nüìù Result: {generated_text}\")\n",
        "print(f\"‚ö° Time: {generation_time:.2f}s\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"‚úÖ Demo complete! Your GPU is working.\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ What You Just Learned\n",
        "\n",
        "You successfully:\n",
        "1. Loaded a model\n",
        "2. Moved it to GPU explicitly (`.to(device)`)\n",
        "3. Verified GPU placement\n",
        "4. Monitored GPU memory\n",
        "5. Ran inference on GPU\n",
        "\n",
        "**This pattern applies to ANY model you want to showcase!**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3: Making It Interactive üé®\n",
        "\n",
        "Great launchables aren't just code - they're **engaging experiences**. Let's add:\n",
        "- User-editable parameters\n",
        "- Visual feedback\n",
        "- Timing metrics\n",
        "- Clear outputs\n",
        "\n",
        "**In this section, you'll:** Build an interactive sentiment analyzer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive sentiment analysis demo\n",
        "\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üé≠ Interactive Sentiment Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load pipeline on GPU (suppressing verbose output)\n",
        "device_id = 0 if torch.cuda.is_available() else -1\n",
        "classifier = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device=device_id\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model loaded on: {'GPU' if device_id == 0 else 'CPU'}\")\n",
        "\n",
        "# User-editable test cases\n",
        "test_texts = [\n",
        "    \"Launchables make sharing AI work so easy!\",\n",
        "    \"Setup took forever and nothing worked.\",\n",
        "    \"The tutorial is clear and examples run perfectly.\",\n",
        "]\n",
        "\n",
        "print(\"\\nüí° Try editing test_texts above and rerunning this cell!\\n\")\n",
        "\n",
        "# Analyze each text\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"Text {i}: \\\"{text}\\\"\")\n",
        "    \n",
        "    start = time.time()\n",
        "    result = classifier(text)[0]\n",
        "    inference_time = time.time() - start\n",
        "    \n",
        "    # Visual output\n",
        "    sentiment = result['label']\n",
        "    confidence = result['score'] * 100\n",
        "    emoji = \"üòä\" if sentiment == \"POSITIVE\" else \"üòû\"\n",
        "    \n",
        "    print(f\"  {emoji} {sentiment} ({confidence:.1f}% confident)\")\n",
        "    print(f\"  ‚ö° {inference_time*1000:.0f}ms\\n\")\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"‚úÖ Interactive demo complete!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ What You Just Learned\n",
        "\n",
        "You built an interactive demo with:\n",
        "1. **User-editable parameters** - Edit `test_texts` and rerun\n",
        "2. **Visual feedback** - Emojis and formatted output\n",
        "3. **Timing metrics** - Shows inference speed\n",
        "4. **Clear results** - Easy to understand\n",
        "\n",
        "**This pattern makes your launchables engaging and shareable!**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4: Best Practices ‚≠ê\n",
        "\n",
        "## Essential Patterns for Great Launchables\n",
        "\n",
        "### ‚úÖ Always Include\n",
        "\n",
        "- [ ] **GPU verification first** - Users need to know if GPU is available\n",
        "- [ ] **Target hardware specs** - State which GPU your demo is optimized for\n",
        "- [ ] **Explicit device placement** - Always use `.to(device)`\n",
        "- [ ] **Clear outputs** - Show what's happening at each step\n",
        "- [ ] **Timing metrics** - Prove GPU acceleration works\n",
        "- [ ] **User invitation** - \"Try changing X to see Y\"\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "print(\"üéØ This demo optimized for: 1x NVIDIA A100 40GB\")\n",
        "print(\"üí° Works on: L40s, A100, H100, or better\")\n",
        "```\n",
        "\n",
        "### ‚úÖ Structure Your Launchable\n",
        "\n",
        "1. **Title and intro** - What will users learn?\n",
        "2. **Installation** - One cell with all dependencies\n",
        "3. **GPU verification** - Verify hardware\n",
        "4. **Simple demo first** - Show it works immediately\n",
        "5. **Interactive demo** - Let users experiment\n",
        "6. **Next steps** - How to build their own\n",
        "\n",
        "### ‚úÖ File Requirements\n",
        "\n",
        "Your launchable repository needs:\n",
        "```\n",
        "your-launchable/\n",
        "‚îú‚îÄ‚îÄ README.md              # Short overview (< 200 words)\n",
        "‚îú‚îÄ‚îÄ requirements.txt       # All pip dependencies\n",
        "‚îú‚îÄ‚îÄ your-notebook.ipynb   # Self-contained tutorial\n",
        "‚îî‚îÄ‚îÄ .gitignore            # Standard Python/Jupyter\n",
        "```\n",
        "\n",
        "### ‚úÖ Common Mistakes to Avoid\n",
        "\n",
        "- ‚ùå **Don't** assume packages are installed\n",
        "- ‚ùå **Don't** use terminal instructions (Brev opens notebook directly)\n",
        "- ‚ùå **Don't** make it too complex (keep under 20 cells)\n",
        "- ‚ùå **Don't** forget to test on fresh kernel\n",
        "- ‚ùå **Don't** use \"production\" language (call it \"shareable prototype\")\n",
        "\n",
        "### ‚úÖ Language Guidelines\n",
        "\n",
        "**Instead of:**\n",
        "- \"Production-ready deployment\" ‚Üí \"Shareable prototype\"\n",
        "- \"Enterprise-grade\" ‚Üí \"Working demo\"\n",
        "- \"Build AI systems\" ‚Üí \"Share your AI work\"\n",
        "\n",
        "**Target audience:**\n",
        "- \"Developers who built something in AI\"\n",
        "- \"Help others build 10x faster\"\n",
        "- \"Share your work with the community\"\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5: Sharing Your Launchable üì§\n",
        "\n",
        "Ready to share your launchable with the world? Here's how.\n",
        "\n",
        "## Step 1: Prepare Your Repository\n",
        "\n",
        "**Create these files:**\n",
        "\n",
        "### `requirements.txt`\n",
        "```txt\n",
        "torch>=2.0.0\n",
        "transformers>=4.30.0\n",
        "accelerate>=0.20.0\n",
        "```\n",
        "\n",
        "### `README.md`\n",
        "```markdown\n",
        "# Your Launchable Title\n",
        "\n",
        "One-sentence description of what this showcases.\n",
        "\n",
        "## What You'll Build\n",
        "- Bullet point 1\n",
        "- Bullet point 2\n",
        "\n",
        "## Prerequisites\n",
        "- GPU-enabled environment\n",
        "- Python 3.8+\n",
        "\n",
        "## Get Started\n",
        "Click \"Open Notebook\" to start.\n",
        "```\n",
        "\n",
        "### `.gitignore`\n",
        "```gitignore\n",
        "# Python\n",
        "__pycache__/\n",
        "*.pyc\n",
        "\n",
        "# Jupyter\n",
        ".ipynb_checkpoints/\n",
        "\n",
        "# Models\n",
        "*.pt\n",
        "*.pth\n",
        "*.bin\n",
        "```\n",
        "\n",
        "## Step 2: Push to GitHub\n",
        "\n",
        "```bash\n",
        "git init\n",
        "git add .\n",
        "git commit -m \"Create launchable for [your project]\"\n",
        "git remote add origin https://github.com/yourusername/your-launchable.git\n",
        "git push -u origin main\n",
        "```\n",
        "\n",
        "## Step 3: Choose Your GPU Configuration\n",
        "\n",
        "**Important:** Launchables are fully configurable to any single-node GPU. Choose the **smallest GPU that runs your demo smoothly**.\n",
        "\n",
        "### Available GPU Options\n",
        "\n",
        "Brev supports configurations across multiple GPU types:\n",
        "- **B200** - Latest NVIDIA Blackwell architecture\n",
        "- **H200** - High-performance Hopper with HBM3e\n",
        "- **H100** - Standard Hopper for demanding workloads\n",
        "- **A100** - Versatile Ampere for most use cases\n",
        "- **L40s** - Cost-effective for inference and visualization\n",
        "\n",
        "**Configurations:** 1x, 2x, 4x, or 8x GPUs per instance\n",
        "\n",
        "üìä **See full list:** [Brev Compute Options](https://brev.nvidia.com/compute)\n",
        "\n",
        "### GPU Selection Best Practices\n",
        "\n",
        "**‚úÖ Start Small (Recommended)**\n",
        "\n",
        "**For most launchables:**\n",
        "- **1x L40s** or **1x A100 (40GB)** - Perfect for demos, tutorials, small models\n",
        "- **Cost-effective** - Lower credits per hour\n",
        "- **Fast startup** - More availability\n",
        "- **Sufficient** - Most demos don't need massive compute\n",
        "\n",
        "**When to scale up:**\n",
        "- **2x-4x GPUs** - Only if your demo actually uses multiple GPUs (distributed training, large batch inference)\n",
        "- **8x GPUs** - AVOID unless absolutely necessary (rare for demos)\n",
        "\n",
        "### üéØ Match GPU to Use Case\n",
        "\n",
        "| Your Launchable Shows | Recommended GPU |\n",
        "|----------------------|-----------------|\n",
        "| Small models (< 7B params) | 1x L40s or 1x A100 40GB |\n",
        "| Medium models (7-13B params) | 1x A100 80GB or 1x H100 |\n",
        "| Large models (13-70B params) | 1x H100 or 1x H200 |\n",
        "| Multi-GPU techniques | 2x or 4x matching your demo |\n",
        "| Cutting-edge features | 1x B200 |\n",
        "\n",
        "**üí° Pro Tip:** Include your target hardware in the notebook! \n",
        "\n",
        "```python\n",
        "print(\"üéØ This demo optimized for: 1x NVIDIA A100 40GB\")\n",
        "print(\"üí° Works on: L40s, A100, H100, or better\")\n",
        "```\n",
        "\n",
        "### Understanding Credits & Cost\n",
        "\n",
        "**How it works:**\n",
        "- **Self-service** - No fixed subscription period\n",
        "- **Credit-based** - Purchase credits, use as needed\n",
        "- **Auto-stop** - Instance stops when credits run out\n",
        "- **Add anytime** - Top up credits to continue\n",
        "- **Pay for usage** - Only charged when running\n",
        "\n",
        "**Cost optimization:**\n",
        "- Smaller GPUs = fewer credits per hour\n",
        "- Efficient code = shorter runtime = lower cost\n",
        "- Clear instructions = users don't waste credits debugging\n",
        "\n",
        "## Step 4: Deploy on Brev\n",
        "\n",
        "1. **Go to** [brev.nvidia.com](https://brev.nvidia.com)\n",
        "2. **Connect** your GitHub repository\n",
        "3. **Select GPU configuration** (remember: start with 1x L40s or 1x A100)\n",
        "4. **Launch** and test your launchable\n",
        "5. **Verify** it runs smoothly on your chosen GPU\n",
        "6. **Share** the launch link with your community!\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **Brev Documentation**: [docs.nvidia.com/brev/latest/launchables.html](https://docs.nvidia.com/brev/latest/launchables.html)\n",
        "- **Launchables Ecosystem**: [github.com/brevdev/launchables](https://github.com/brevdev/launchables)\n",
        "- **Examples**: Browse existing launchables for inspiration\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 6: Your Challenge üéØ\n",
        "\n",
        "## Build Your Own Launchable!\n",
        "\n",
        "You now have everything you need. Here are **concrete examples** to get you started:\n",
        "\n",
        "### Example 1: \"2x Faster Fine-Tuning\" Demo (Unsloth-style)\n",
        "\n",
        "**What to show:**\n",
        "```python\n",
        "# Load model with YOUR optimization\n",
        "model = YourOptimizedLoader(\"llama-7b\")\n",
        "\n",
        "# Time YOUR method\n",
        "start = time.time()\n",
        "model.train(sample_data)\n",
        "your_time = time.time() - start\n",
        "\n",
        "# Time standard method\n",
        "baseline_model = HuggingFaceLoader(\"llama-7b\")\n",
        "start = time.time()\n",
        "baseline_model.train(sample_data)\n",
        "baseline_time = time.time() - start\n",
        "\n",
        "# Show the speedup\n",
        "print(f\"Your method: {your_time:.2f}s\")\n",
        "print(f\"Baseline: {baseline_time:.2f}s\")\n",
        "print(f\"üöÄ {baseline_time/your_time:.1f}x FASTER!\")\n",
        "```\n",
        "\n",
        "**Result:** Developers SEE your speedup in 60 seconds.\n",
        "\n",
        "### Example 2: \"Side-by-Side Config Comparison\" (Axolotl-style)\n",
        "\n",
        "**What to show:**\n",
        "- Load same model with 3 different configs\n",
        "- Train for 10 steps each\n",
        "- Show loss curves side-by-side with matplotlib\n",
        "- Display memory usage for each config\n",
        "\n",
        "**Result:** Developers understand which config to use.\n",
        "\n",
        "### Example 3: \"Novel Architecture Demo\"\n",
        "\n",
        "**What to show:**\n",
        "- Load your architecture + baseline architecture\n",
        "- Run same task on both\n",
        "- Compare: inference time, memory, accuracy\n",
        "- Display results in a table\n",
        "\n",
        "**Result:** Developers see WHY your architecture matters.\n",
        "\n",
        "### Checklist: Converting Your Existing Notebook\n",
        "\n",
        "Take a notebook you already have:\n",
        "\n",
        "- [ ] **Add Cell 1:** Installation (`pip install` all dependencies)\n",
        "- [ ] **Add Cell 2:** GPU verification (copy from this tutorial)\n",
        "- [ ] **Update code:** Add explicit `.to(device)` for all models\n",
        "- [ ] **Add timing:** Wrap key operations with `time.time()`\n",
        "- [ ] **Add comparison:** Show YOUR method vs baseline\n",
        "- [ ] **Add visuals:** Print results clearly, use emojis for status\n",
        "- [ ] **Test:** Run all cells on fresh kernel\n",
        "- [ ] **Create README:** Copy template from Section 5\n",
        "- [ ] **Push to GitHub:** `git push`\n",
        "- [ ] **Deploy on Brev:** Connect repo, select GPU, launch\n",
        "\n",
        "### How Developers Will Find Your Launchable\n",
        "\n",
        "**Distribution options:**\n",
        "\n",
        "1. **Direct link** - Share on Twitter, Discord, docs\n",
        "   - \"Try our 2x faster fine-tuning: [brev.nvidia.com/your-launchable]\"\n",
        "   \n",
        "2. **Brev showcase** - Featured in launchables directory\n",
        "   - Discoverable by developers browsing GPU tools\n",
        "   \n",
        "3. **Your website/docs** - Embed \"Launch Demo\" button\n",
        "   - Users can try before they pip install\n",
        "   \n",
        "4. **GitHub README** - Add badge at the top\n",
        "   - \"üöÄ Try this in 30 seconds\" ‚Üí instant credibility\n",
        "\n",
        "**Best practice:** Post launch link when you announce your tool. \"Here's our new optimization technique - try it live in 60 seconds [link]\"\n",
        "\n",
        "### Join the Ecosystem\n",
        "\n",
        "When your launchable is ready:\n",
        "- **Tweet it:** \"#BrevLaunchables - Try [your tool] with zero setup\"\n",
        "- **Add to showcase:** [github.com/brevdev/launchables](https://github.com/brevdev/launchables)\n",
        "- **Link in README:** Help others discover your work\n",
        "- **Share here:** Help other devs with their launchables\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've completed the tutorial. You now know how to:\n",
        "- ‚úÖ Verify GPU availability\n",
        "- ‚úÖ Build interactive demos\n",
        "- ‚úÖ Follow best practices\n",
        "- ‚úÖ Share your work as a launchable\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Meta Moment\n",
        "\n",
        "**Notice something?** \n",
        "\n",
        "**THIS notebook you just completed IS ITSELF a launchable!**\n",
        "\n",
        "You:\n",
        "- Clicked \"Open Notebook\" and landed here\n",
        "- Ran ONE install cell\n",
        "- Restarted the kernel\n",
        "- Ran all cells successfully\n",
        "- Learned in a GPU-backed environment\n",
        "- Can share this exact link with others\n",
        "\n",
        "**That's the power of launchables.**\n",
        "\n",
        "Now you can create the same experience for YOUR content!\n",
        "\n",
        "---\n",
        "\n",
        "**Go make your AI work discoverable and help others be 10x faster!** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*Tutorial for developers who want to share their AI work*  \n",
        "*Making AI innovations accessible, one launchable at a time* üíö\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ How to Build NVIDIA Launchables\n",
        "## *The Definitive Interactive Tutorial*\n",
        "\n",
        "Welcome to the **meta-launchable** - a tutorial that teaches you how to build launchables while demonstrating best practices through its own implementation!\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Build Today\n",
        "\n",
        "By the end of this tutorial, you'll have:\n",
        "- ‚úÖ A working understanding of the launchables pattern\n",
        "- ‚úÖ A GPU-accelerated demo application\n",
        "- ‚úÖ The skills to create and deploy your own launchables\n",
        "- ‚úÖ A complete workflow from idea to production\n",
        "\n",
        "### How This Works\n",
        "\n",
        "This notebook is **interactive** - you'll run every cell, see real outputs, and build alongside the tutorial. Think of it as pair programming with Jensen Huang's vision of democratizing AI.\n",
        "\n",
        "> **üí° Pro Tip**: Read each section carefully, run the code cells sequentially, and complete the exercises. By the end, you'll have built your first launchable!\n",
        "\n",
        "---\n",
        "\n",
        "### üìã Quick Navigation\n",
        "\n",
        "1. **Introduction & GPU Setup** ‚Üê *Start here*\n",
        "2. Understanding the Launchables Structure\n",
        "3. GPU-First Development\n",
        "4. Building Interactive Demos\n",
        "5. Prompt Engineering with Claude\n",
        "6. Debugging & Testing\n",
        "7. Git & GitHub Setup\n",
        "8. Deploying to Brev\n",
        "9. Your First Launchable Exercise\n",
        "10. Resources & Next Steps\n",
        "\n",
        "---\n",
        "\n",
        "**Ready?** Let's verify your GPU and get started! üëá\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 1: Introduction & GPU Setup üéØ\n",
        "\n",
        "## What is a Launchable?\n",
        "\n",
        "A **launchable** is a self-contained, GPU-accelerated, interactive notebook that:\n",
        "- üì¶ **Packages everything** - Code, docs, and dependencies in one place\n",
        "- üöÄ **Runs instantly** - No complex setup required\n",
        "- üíª **Uses GPU** - Accelerates AI/ML workloads\n",
        "- üåê **Deploys easily** - One-click deployment to Brev\n",
        "- üéì **Teaches by doing** - Interactive, hands-on learning\n",
        "\n",
        "Think of it as a \"runnable README\" for AI projects.\n",
        "\n",
        "## Why Build Launchables?\n",
        "\n",
        "1. **Share Your Work** - Make your AI projects accessible to everyone\n",
        "2. **Learn Faster** - Best way to learn is by building and sharing\n",
        "3. **Build Portfolio** - Showcase your skills with deployable projects\n",
        "4. **Contribute** - Help democratize AI development\n",
        "\n",
        "## The Launchables Ecosystem\n",
        "\n",
        "- **Repository**: [github.com/brevdev/launchables](https://github.com/brevdev/launchables)\n",
        "- **Platform**: [Brev.dev](https://brev.dev) - One-click GPU deployment\n",
        "- **Community**: Contributors building the future of AI tooling\n",
        "\n",
        "---\n",
        "\n",
        "## üî• CRITICAL: GPU Verification\n",
        "\n",
        "**This is the most important cell in any launchable!**\n",
        "\n",
        "Before we go further, we MUST verify that:\n",
        "1. A GPU is available\n",
        "2. CUDA is properly installed\n",
        "3. PyTorch can access the GPU\n",
        "4. We know what hardware we're working with\n",
        "\n",
        "Run the cell below ‚¨áÔ∏è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "üî• CRITICAL GPU VERIFICATION\n",
        "This cell MUST be the first executable cell in every launchable!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîç GPU VERIFICATION REPORT\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check Python version\n",
        "print(f\"\\nüìå Python Version: {sys.version.split()[0]}\")\n",
        "\n",
        "# Check PyTorch version\n",
        "print(f\"üìå PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "# Check CUDA availability\n",
        "cuda_available = torch.cuda.is_available()\n",
        "print(f\"\\n{'‚úÖ' if cuda_available else '‚ùå'} CUDA Available: {cuda_available}\")\n",
        "\n",
        "if cuda_available:\n",
        "    # GPU Details\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"‚úÖ Number of GPUs: {gpu_count}\")\n",
        "    \n",
        "    for i in range(gpu_count):\n",
        "        print(f\"\\nüìä GPU {i} Details:\")\n",
        "        print(f\"   Name: {torch.cuda.get_device_name(i)}\")\n",
        "        print(f\"   Compute Capability: {torch.cuda.get_device_capability(i)}\")\n",
        "        \n",
        "        # Memory info\n",
        "        total_memory = torch.cuda.get_device_properties(i).total_memory / 1e9\n",
        "        print(f\"   Total Memory: {total_memory:.2f} GB\")\n",
        "        \n",
        "        # Current memory usage\n",
        "        allocated = torch.cuda.memory_allocated(i) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(i) / 1e9\n",
        "        print(f\"   Allocated Memory: {allocated:.2f} GB\")\n",
        "        print(f\"   Reserved Memory: {reserved:.2f} GB\")\n",
        "    \n",
        "    # Test GPU with a simple operation\n",
        "    print(\"\\nüß™ Testing GPU with sample tensor operation...\")\n",
        "    test_tensor = torch.randn(1000, 1000).cuda()\n",
        "    result = torch.matmul(test_tensor, test_tensor)\n",
        "    print(f\"‚úÖ GPU test successful! Result shape: {result.shape}\")\n",
        "    print(f\"‚úÖ Tensor is on device: {result.device}\")\n",
        "    \n",
        "    # Clean up\n",
        "    del test_tensor, result\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üéâ SUCCESS! Your GPU is ready for AI development!\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "else:\n",
        "    # Fallback message\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nüîß Troubleshooting Steps:\")\n",
        "    print(\"1. Verify nvidia-smi works: Run 'nvidia-smi' in terminal\")\n",
        "    print(\"2. Check CUDA installation: Visit https://developer.nvidia.com/cuda-downloads\")\n",
        "    print(\"3. Reinstall PyTorch with CUDA: https://pytorch.org/get-started/locally/\")\n",
        "    print(\"4. Verify GPU drivers are up to date\")\n",
        "    print(\"\\nüí° Common Issues:\")\n",
        "    print(\"   - Wrong PyTorch version (CPU-only)\")\n",
        "    print(\"   - CUDA version mismatch\")\n",
        "    print(\"   - GPU drivers not installed\")\n",
        "    print(\"\\n‚ö†Ô∏è  This launchable requires a GPU to run properly.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Set default device for rest of notebook\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"\\nüéØ Default device set to: {device}\")\n",
        "print(f\"‚úÖ All future operations will use: {device.type.upper()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup Checklist\n",
        "\n",
        "Before we continue, make sure you have:\n",
        "\n",
        "**Required:**\n",
        "- ‚úÖ GPU detected (verified above)\n",
        "- ‚úÖ PyTorch with CUDA support installed\n",
        "- ‚úÖ Jupyter notebook running\n",
        "- ‚úÖ Git installed (`git --version` in terminal)\n",
        "- ‚úÖ GitHub account created\n",
        "\n",
        "**Recommended:**\n",
        "- üìù Code editor (VSCode, Cursor, or similar)\n",
        "- üêô Git configured with SSH keys\n",
        "- üåê Brev.dev account (for deployment later)\n",
        "\n",
        "### Quick Environment Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Quick environment check - verify all key dependencies\n",
        "\"\"\"\n",
        "\n",
        "import subprocess\n",
        "import importlib\n",
        "\n",
        "def check_import(package_name, display_name=None):\n",
        "    \"\"\"Check if a package can be imported and get its version\"\"\"\n",
        "    if display_name is None:\n",
        "        display_name = package_name\n",
        "    try:\n",
        "        module = importlib.import_module(package_name)\n",
        "        version = getattr(module, '__version__', 'unknown')\n",
        "        print(f\"‚úÖ {display_name}: {version}\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"‚ùå {display_name}: Not installed\")\n",
        "        return False\n",
        "\n",
        "def check_command(command, name):\n",
        "    \"\"\"Check if a command is available\"\"\"\n",
        "    try:\n",
        "        result = subprocess.run([command, '--version'], \n",
        "                              capture_output=True, text=True, timeout=5)\n",
        "        version_line = result.stdout.split('\\n')[0] if result.stdout else result.stderr.split('\\n')[0]\n",
        "        print(f\"‚úÖ {name}: {version_line}\")\n",
        "        return True\n",
        "    except (subprocess.TimeoutExpired, FileNotFoundError):\n",
        "        print(f\"‚ùå {name}: Not found\")\n",
        "        return False\n",
        "\n",
        "print(\"üîç Checking Dependencies...\\n\")\n",
        "\n",
        "# Python packages\n",
        "check_import('torch', 'PyTorch')\n",
        "check_import('transformers', 'Transformers')\n",
        "check_import('numpy', 'NumPy')\n",
        "check_import('matplotlib', 'Matplotlib')\n",
        "\n",
        "print()\n",
        "\n",
        "# System tools\n",
        "check_command('git', 'Git')\n",
        "check_command('nvidia-smi', 'nvidia-smi')\n",
        "\n",
        "print(\"\\n‚úÖ Environment check complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 2: Understanding the Launchables Structure üìÅ\n",
        "\n",
        "## The Launchables Pattern\n",
        "\n",
        "A well-structured launchable follows this pattern:\n",
        "\n",
        "```\n",
        "your-launchable/\n",
        "‚îú‚îÄ‚îÄ README.md                 # Overview, prerequisites, quick start\n",
        "‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies with versions\n",
        "‚îú‚îÄ‚îÄ .gitignore               # Exclude cache, models, etc.\n",
        "‚îú‚îÄ‚îÄ main-notebook.ipynb      # Your interactive tutorial\n",
        "‚îî‚îÄ‚îÄ (optional) assets/       # Images, data files, etc.\n",
        "```\n",
        "\n",
        "### Why This Structure?\n",
        "\n",
        "1. **README.md** - First thing people see. Must be compelling!\n",
        "2. **requirements.txt** - Reproducible environment setup\n",
        "3. **.gitignore** - Keep repo clean (no model checkpoints!)\n",
        "4. **Notebook** - Self-contained learning experience\n",
        "5. **Assets** - Supporting materials (keep them small!)\n",
        "\n",
        "## Examples from the Ecosystem\n",
        "\n",
        "Let's look at real launchables:\n",
        "\n",
        "### Example 1: Model Fine-tuning\n",
        "```\n",
        "fine-tune-llama/\n",
        "‚îú‚îÄ‚îÄ README.md              # \"Fine-tune Llama 2 in 30 minutes\"\n",
        "‚îú‚îÄ‚îÄ requirements.txt       # torch, transformers, datasets, peft\n",
        "‚îú‚îÄ‚îÄ fine-tune.ipynb       # Step-by-step tutorial\n",
        "‚îî‚îÄ‚îÄ sample-data/          # Small example dataset\n",
        "```\n",
        "\n",
        "### Example 2: Production Deployment\n",
        "```\n",
        "vllm-production/\n",
        "‚îú‚îÄ‚îÄ README.md              # \"Deploy LLMs at scale\"\n",
        "‚îú‚îÄ‚îÄ requirements.txt       # vllm, fastapi, uvicorn\n",
        "‚îú‚îÄ‚îÄ deployment.ipynb      # Interactive setup guide\n",
        "‚îî‚îÄ‚îÄ config/               # Sample configurations\n",
        "```\n",
        "\n",
        "### Example 3: This Tutorial!\n",
        "```\n",
        "how-to-build-launchables/\n",
        "‚îú‚îÄ‚îÄ README.md              # What you're learning\n",
        "‚îú‚îÄ‚îÄ requirements.txt       # All dependencies\n",
        "‚îú‚îÄ‚îÄ .gitignore            # Clean repo\n",
        "‚îî‚îÄ‚îÄ how-to-build-launchables.ipynb  # This file!\n",
        "```\n",
        "\n",
        "## Best Practices\n",
        "\n",
        "### ‚úÖ DO:\n",
        "- Keep notebooks focused (1-2 hours to complete)\n",
        "- Include working code examples\n",
        "- Test on fresh environment before sharing\n",
        "- Add clear error messages\n",
        "- Use GPU verification at start\n",
        "- Include progress indicators\n",
        "\n",
        "### ‚ùå DON'T:\n",
        "- Commit large model files (use `.gitignore`)\n",
        "- Hardcode personal paths or tokens\n",
        "- Skip GPU verification\n",
        "- Make assumptions about environment\n",
        "- Leave broken cells\n",
        "- Forget to test end-to-end\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Exercise: Understanding Structure\n",
        "\n",
        "Look at this repository's structure. Can you identify:\n",
        "1. Where are the dependencies listed?\n",
        "2. What files are ignored by git?\n",
        "3. How is this notebook organized?\n",
        "\n",
        "**Answer**: Use `!ls -la` to explore!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explore the repository structure\n",
        "import os\n",
        "\n",
        "print(\"üìÅ Current Directory Structure:\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# List files in current directory\n",
        "files = os.listdir('.')\n",
        "files.sort()\n",
        "\n",
        "for file in files:\n",
        "    if file.startswith('.'):\n",
        "        icon = \"üîí\"  # Hidden file\n",
        "    elif file.endswith('.ipynb'):\n",
        "        icon = \"üìì\"\n",
        "    elif file.endswith('.md'):\n",
        "        icon = \"üìù\"\n",
        "    elif file.endswith('.txt'):\n",
        "        icon = \"üìÑ\"\n",
        "    elif file.endswith('.py'):\n",
        "        icon = \"üêç\"\n",
        "    elif os.path.isdir(file):\n",
        "        icon = \"üìÇ\"\n",
        "    else:\n",
        "        icon = \"üìÑ\"\n",
        "    \n",
        "    size = \"DIR\" if os.path.isdir(file) else f\"{os.path.getsize(file):,} bytes\"\n",
        "    print(f\"{icon} {file:<40} {size}\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüí° Notice:\")\n",
        "print(\"   - requirements.txt defines our dependencies\")\n",
        "print(\"   - .gitignore keeps repo clean\")\n",
        "print(\"   - This notebook is self-contained\")\n",
        "print(\"   - README.md provides overview\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 3: GPU-First Development üî•\n",
        "\n",
        "## Why GPU-First Matters\n",
        "\n",
        "The #1 mistake in AI development: **Assuming code runs on GPU when it doesn't!**\n",
        "\n",
        "Your code might:\n",
        "- ‚úÖ Run successfully (no errors)\n",
        "- ‚úÖ Produce correct results\n",
        "- ‚ùå But run 100x slower on CPU!\n",
        "\n",
        "## The GPU Development Checklist\n",
        "\n",
        "For EVERY operation with neural networks:\n",
        "\n",
        "1. **Verify device at model load time**\n",
        "2. **Verify device during inference**\n",
        "3. **Monitor GPU memory usage**\n",
        "4. **Check GPU utilization** (is it actually working?)\n",
        "5. **Handle device mismatches gracefully**\n",
        "\n",
        "## Device Management Pattern\n",
        "\n",
        "Here's the pattern you should use in EVERY launchable:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GOLD STANDARD: Device Management Pattern\n",
        "Use this in every launchable!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "# 1. Detect and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üéØ Using device: {device}\")\n",
        "\n",
        "# 2. Check GPU properties if available\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Running on CPU - this will be slower!\")\n",
        "\n",
        "# 3. Create tensors on the correct device\n",
        "# Method 1: Create then move\n",
        "tensor1 = torch.randn(100, 100).to(device)\n",
        "print(f\"\\nüìä Tensor 1 device: {tensor1.device}\")\n",
        "\n",
        "# Method 2: Create directly on device\n",
        "tensor2 = torch.randn(100, 100, device=device)\n",
        "print(f\"üìä Tensor 2 device: {tensor2.device}\")\n",
        "\n",
        "# 4. Verify operations stay on GPU\n",
        "result = torch.matmul(tensor1, tensor2)\n",
        "print(f\"üìä Result device: {result.device}\")\n",
        "\n",
        "# 5. Check memory usage (GPU only)\n",
        "if device.type == \"cuda\":\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e6\n",
        "    print(f\"\\nüíæ GPU Memory allocated: {allocated:.2f} MB\")\n",
        "\n",
        "print(\"\\n‚úÖ Device management verified!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common GPU Pitfalls and Solutions\n",
        "\n",
        "### ‚ùå Pitfall 1: Model on GPU, Data on CPU\n",
        "\n",
        "```python\n",
        "# BAD: Model and data on different devices\n",
        "model = MyModel().cuda()\n",
        "data = torch.randn(10, 10)  # Still on CPU!\n",
        "output = model(data)  # ERROR: device mismatch\n",
        "```\n",
        "\n",
        "```python\n",
        "# GOOD: Everything on same device\n",
        "model = MyModel().to(device)\n",
        "data = torch.randn(10, 10, device=device)\n",
        "output = model(data)  # Works!\n",
        "```\n",
        "\n",
        "### ‚ùå Pitfall 2: Not Checking GPU Utilization\n",
        "\n",
        "Just because your code runs doesn't mean it's using the GPU!\n",
        "\n",
        "**Always verify with `nvidia-smi`:**\n",
        "```bash\n",
        "# In terminal, run:\n",
        "watch -n 1 nvidia-smi\n",
        "\n",
        "# Look for:\n",
        "# - GPU Utilization > 0%\n",
        "# - Memory Usage increasing\n",
        "# - Your Python process listed\n",
        "```\n",
        "\n",
        "### ‚ùå Pitfall 3: Forgetting to Clear Cache\n",
        "\n",
        "```python\n",
        "# BAD: Memory leaks over time\n",
        "for i in range(100):\n",
        "    big_tensor = torch.randn(1000, 1000, device='cuda')\n",
        "    # Tensor never freed!\n",
        "```\n",
        "\n",
        "```python\n",
        "# GOOD: Explicit cleanup\n",
        "for i in range(100):\n",
        "    big_tensor = torch.randn(1000, 1000, device='cuda')\n",
        "    result = process(big_tensor)\n",
        "    del big_tensor  # Free memory\n",
        "    if i % 10 == 0:\n",
        "        torch.cuda.empty_cache()  # Clear cache periodically\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Live Demo: Load a Real Model on GPU\n",
        "\n",
        "Let's load a small but real model and verify GPU usage at every step!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Live Demo: Load DistilGPT-2 on GPU\n",
        "This is a small model perfect for learning!\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "print(\"üîÑ Loading DistilGPT-2 model...\\n\")\n",
        "\n",
        "# Step 1: Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üìç Target device: {device}\")\n",
        "\n",
        "# Step 2: Load tokenizer (always on CPU)\n",
        "print(\"\\nüîÑ Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "print(\"‚úÖ Tokenizer loaded\")\n",
        "\n",
        "# Step 3: Load model and move to GPU\n",
        "print(\"\\nüîÑ Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "print(f\"‚úÖ Model loaded (currently on: CPU)\")\n",
        "\n",
        "print(\"\\nüîÑ Moving model to GPU...\")\n",
        "model = model.to(device)\n",
        "print(f\"‚úÖ Model moved to: {device}\")\n",
        "\n",
        "# Step 4: Verify model is on GPU\n",
        "print(\"\\nüîç Verification:\")\n",
        "print(f\"   Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Step 5: Check memory usage\n",
        "if device.type == \"cuda\":\n",
        "    memory_allocated = torch.cuda.memory_allocated(0) / 1e6\n",
        "    print(f\"   GPU Memory used: {memory_allocated:.2f} MB\")\n",
        "    \n",
        "    # Get GPU utilization using nvidia-smi\n",
        "    print(\"\\nüí° Tip: Open a terminal and run 'nvidia-smi' to see:\")\n",
        "    print(\"   - This process using GPU memory\")\n",
        "    print(\"   - Current GPU utilization\")\n",
        "\n",
        "print(\"\\n‚úÖ Model successfully loaded on GPU!\")\n",
        "print(\"\\nüéØ Next: Let's use this model to generate text...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generate text with GPU acceleration\n",
        "Watch the memory usage change!\n",
        "\"\"\"\n",
        "\n",
        "print(\"üéØ Generating text on GPU...\\n\")\n",
        "\n",
        "# Step 1: Prepare input\n",
        "text = \"The future of AI is\"\n",
        "print(f\"üìù Input: '{text}'\")\n",
        "\n",
        "# Step 2: Tokenize and move to device\n",
        "print(\"\\nüîÑ Tokenizing...\")\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(f\"   Input tokens shape: {inputs['input_ids'].shape}\")\n",
        "print(f\"   Currently on: {inputs['input_ids'].device}\")\n",
        "\n",
        "# Step 3: Move inputs to same device as model\n",
        "print(\"\\nüîÑ Moving inputs to GPU...\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "print(f\"   Inputs now on: {inputs['input_ids'].device}\")\n",
        "\n",
        "# Step 4: Check memory before generation\n",
        "if device.type == \"cuda\":\n",
        "    memory_before = torch.cuda.memory_allocated(0) / 1e6\n",
        "    print(f\"\\nüíæ GPU Memory before generation: {memory_before:.2f} MB\")\n",
        "\n",
        "# Step 5: Generate text\n",
        "print(\"\\nüöÄ Generating (this happens on GPU)...\")\n",
        "with torch.no_grad():  # Disable gradient computation for inference\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "# Step 6: Verify outputs are on GPU\n",
        "print(f\"‚úÖ Generation complete!\")\n",
        "print(f\"   Output tensor device: {outputs.device}\")\n",
        "\n",
        "# Step 7: Check memory after generation\n",
        "if device.type == \"cuda\":\n",
        "    memory_after = torch.cuda.memory_allocated(0) / 1e6\n",
        "    print(f\"   GPU Memory after generation: {memory_after:.2f} MB\")\n",
        "    print(f\"   Memory used during generation: {memory_after - memory_before:.2f} MB\")\n",
        "\n",
        "# Step 8: Decode and display result\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nüìÑ Generated text:\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(generated_text)\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\n‚úÖ GPU-accelerated generation complete!\")\n",
        "print(\"üí° This was 10-100x faster than CPU!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 4: Building Interactive Demos üé®\n",
        "\n",
        "## The Art of Interactive Learning\n",
        "\n",
        "A great launchable isn't just code - it's an **experience**. Users should:\n",
        "- üéØ Understand what they're building\n",
        "- üîß Interact with working examples\n",
        "- üí° Learn by experimenting\n",
        "- üéâ Feel accomplished at the end\n",
        "\n",
        "## Elements of a Great Demo\n",
        "\n",
        "### 1. Clear Objectives\n",
        "Tell users what they'll accomplish\n",
        "\n",
        "### 2. Progressive Complexity\n",
        "Start simple, gradually add features\n",
        "\n",
        "### 3. Immediate Feedback\n",
        "Show results right away\n",
        "\n",
        "### 4. Interactivity\n",
        "Let users modify and experiment\n",
        "\n",
        "### 5. Visual Elements\n",
        "Use progress bars, formatting, emojis\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Interactive Demo: Text Generation Playground\n",
        "\n",
        "Let's create an interactive text generation demo where users can experiment!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Interactive Text Generation Playground\n",
        "Users can customize the prompt and parameters!\n",
        "\"\"\"\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "def generate_text_with_options(\n",
        "    prompt,\n",
        "    max_length=50,\n",
        "    temperature=0.7,\n",
        "    num_sequences=1,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate text with customizable parameters\n",
        "    \n",
        "    Args:\n",
        "        prompt: Starting text\n",
        "        max_length: Maximum tokens to generate\n",
        "        temperature: Creativity (0.1=conservative, 1.0=creative)\n",
        "        num_sequences: Number of different generations\n",
        "        verbose: Show progress and details\n",
        "    \"\"\"\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"üéØ Generating with:\")\n",
        "        print(f\"   Prompt: '{prompt}'\")\n",
        "        print(f\"   Max length: {max_length} tokens\")\n",
        "        print(f\"   Temperature: {temperature}\")\n",
        "        print(f\"   Sequences: {num_sequences}\")\n",
        "        print()\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    \n",
        "    # Check GPU usage\n",
        "    if device.type == \"cuda\" and verbose:\n",
        "        mem_before = torch.cuda.memory_allocated(0) / 1e6\n",
        "        print(f\"üíæ GPU Memory: {mem_before:.2f} MB\")\n",
        "    \n",
        "    # Generate\n",
        "    if verbose:\n",
        "        print(\"üöÄ Generating...\\n\")\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            num_return_sequences=num_sequences,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            top_p=0.95,\n",
        "            top_k=50\n",
        "        )\n",
        "    \n",
        "    # Decode results\n",
        "    results = []\n",
        "    for i, output in enumerate(outputs):\n",
        "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
        "        results.append(text)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"üìÑ Result {i+1}:\")\n",
        "            print(\"‚îÄ\" * 70)\n",
        "            print(text)\n",
        "            print(\"‚îÄ\" * 70)\n",
        "            print()\n",
        "    \n",
        "    if device.type == \"cuda\" and verbose:\n",
        "        mem_after = torch.cuda.memory_allocated(0) / 1e6\n",
        "        print(f\"üíæ GPU Memory after: {mem_after:.2f} MB\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "print(\"=\" * 70)\n",
        "print(\"üé® INTERACTIVE TEXT GENERATION PLAYGROUND\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Try it out!\n",
        "results = generate_text_with_options(\n",
        "    prompt=\"Artificial intelligence will transform\",\n",
        "    max_length=60,\n",
        "    temperature=0.8,\n",
        "    num_sequences=2\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Generation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Try It Yourself!\n",
        "\n",
        "**Exercise**: Modify the cell above to generate different text:\n",
        "\n",
        "1. **Change the prompt** - Try: \"In the year 2050,\", \"The best way to learn programming is\", etc.\n",
        "2. **Adjust temperature** - Low (0.3) = focused, High (1.2) = creative\n",
        "3. **Generate multiple** - Set `num_sequences=3` for variety\n",
        "4. **Make it longer** - Increase `max_length` (but watch GPU memory!)\n",
        "\n",
        "**Pro Tips:**\n",
        "- Temperature 0.7-0.9: Balanced creativity\n",
        "- Temperature < 0.5: More factual, repetitive\n",
        "- Temperature > 1.0: Very creative, sometimes incoherent\n",
        "- `top_p=0.95`: Nucleus sampling for quality\n",
        "\n",
        "---\n",
        "\n",
        "## Adding Progress Indicators\n",
        "\n",
        "For longer operations, always show progress!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Demo: Progress indicators for better UX\n",
        "\"\"\"\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "\n",
        "print(\"üéØ Batch Text Generation with Progress Bar\\n\")\n",
        "\n",
        "prompts = [\n",
        "    \"The future of technology\",\n",
        "    \"Machine learning enables\",\n",
        "    \"The most important skill\",\n",
        "]\n",
        "\n",
        "results_list = []\n",
        "\n",
        "# Progress bar for user feedback\n",
        "for prompt in tqdm(prompts, desc=\"Generating\", unit=\"prompt\"):\n",
        "    result = generate_text_with_options(\n",
        "        prompt=prompt,\n",
        "        max_length=40,\n",
        "        temperature=0.7,\n",
        "        num_sequences=1,\n",
        "        verbose=False  # Suppress per-generation output\n",
        "    )\n",
        "    results_list.append((prompt, result[0]))\n",
        "    \n",
        "    # Small delay to see progress bar (remove in production)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, (prompt, result) in enumerate(results_list, 1):\n",
        "    print(f\"\\n{i}. Prompt: '{prompt}'\")\n",
        "    print(f\"   Result: {result[:100]}...\")\n",
        "\n",
        "print(\"\\n‚úÖ Batch generation complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 5: Prompt Engineering with Claude ü§ñ\n",
        "\n",
        "## The AI-Assisted Development Workflow\n",
        "\n",
        "Here's a secret: **The best developers use AI to build AI!**\n",
        "\n",
        "### The Claude ‚Üí Cursor Pipeline\n",
        "\n",
        "1. **Brainstorm with Claude** - Refine your launchable idea\n",
        "2. **Generate Cursor Prompts** - Create detailed implementation prompts\n",
        "3. **Implement with Cursor** - Let AI write the code\n",
        "4. **Debug with Claude** - When stuck, describe the issue\n",
        "5. **Iterate** - Repeat until perfect\n",
        "\n",
        "## How to Create Effective Cursor Prompts\n",
        "\n",
        "### ‚ùå Bad Prompt:\n",
        "```\n",
        "\"Create a launchable for fine-tuning\"\n",
        "```\n",
        "*Too vague - Cursor will guess and probably get it wrong*\n",
        "\n",
        "### ‚úÖ Good Prompt:\n",
        "```\n",
        "Create a NVIDIA Launchable for fine-tuning Llama 2 on custom datasets.\n",
        "\n",
        "Requirements:\n",
        "1. Structure: Jupyter notebook-based launchable in root directory\n",
        "   - Filename: fine-tune-llama2.ipynb\n",
        "   - Include requirements.txt, README.md, .gitignore\n",
        "\n",
        "2. Technical Implementation:\n",
        "   - Use LoRA/QLoRA for efficient fine-tuning\n",
        "   - 4-bit quantization for memory efficiency\n",
        "   - Support custom dataset loading\n",
        "   - GPU verification as first cell\n",
        "   \n",
        "3. Content Flow (5-7 sections):\n",
        "   - Section 1: Setup & GPU verification\n",
        "   - Section 2: Load base model with quantization\n",
        "   - Section 3: Prepare dataset\n",
        "   - Section 4: Configure LoRA\n",
        "   - Section 5: Train with progress monitoring\n",
        "   - Section 6: Evaluate and save\n",
        "   - Section 7: Inference examples\n",
        "\n",
        "4. Quality Requirements:\n",
        "   - All code must run on 16GB GPU\n",
        "   - Include error handling\n",
        "   - Add progress bars for training\n",
        "   - Verify GPU usage throughout\n",
        "   - Test on fresh environment\n",
        "\n",
        "Deliverables:\n",
        "- fine-tune-llama2.ipynb (working notebook)\n",
        "- requirements.txt (pinned versions)\n",
        "- README.md (prerequisites, quick start)\n",
        "```\n",
        "\n",
        "*Specific, actionable, complete!*\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Exercise: Generate Your Own Cursor Prompt\n",
        "\n",
        "Let's use Claude to create a prompt for YOUR launchable idea!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Claude Prompt Template\n",
        "\n",
        "**Copy this template to Claude.ai:**\n",
        "\n",
        "```\n",
        "I want to create a NVIDIA Launchable for [YOUR IDEA].\n",
        "\n",
        "My launchable should:\n",
        "- Target audience: [beginners/intermediate/advanced]\n",
        "- Main goal: [what users will learn/build]\n",
        "- Time to complete: [30 minutes/1 hour/2 hours]\n",
        "- GPU requirements: [4GB/8GB/16GB+ VRAM]\n",
        "\n",
        "Please help me create a detailed Cursor prompt that includes:\n",
        "1. Clear structure and file organization\n",
        "2. Technical requirements and dependencies\n",
        "3. Detailed content flow (sections and what goes in each)\n",
        "4. Quality standards and testing requirements\n",
        "5. GPU-specific considerations\n",
        "\n",
        "The prompt should be so detailed that Cursor can implement it end-to-end.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Real Examples: What Makes Great Launchables\n",
        "\n",
        "### Example 1: \"Fine-tune Stable Diffusion\"\n",
        "- ‚úÖ Clear value prop: \"Train on your own images in 30 min\"\n",
        "- ‚úÖ Reasonable scope: Uses LoRA, not full fine-tuning\n",
        "- ‚úÖ Interactive: Shows results after each epoch\n",
        "- ‚úÖ Practical: Saves model for reuse\n",
        "\n",
        "### Example 2: \"RAG System from Scratch\"\n",
        "- ‚úÖ Teaches concepts: Explains embeddings, vector DBs\n",
        "- ‚úÖ Complete workflow: Ingest ‚Üí Embed ‚Üí Query ‚Üí Generate\n",
        "- ‚úÖ Real use case: Build searchable documentation\n",
        "- ‚úÖ Extensible: Easy to add your own data\n",
        "\n",
        "### Example 3: \"LLM Performance Optimization\"\n",
        "- ‚úÖ Comparative: Shows different optimization techniques\n",
        "- ‚úÖ Benchmarked: Measures speed and memory\n",
        "- ‚úÖ Production-ready: Actual deployment patterns\n",
        "- ‚úÖ Best practices: Error handling, monitoring\n",
        "\n",
        "---\n",
        "\n",
        "## Debugging with Claude\n",
        "\n",
        "When you encounter issues in Cursor:\n",
        "\n",
        "1. **Copy the error message**\n",
        "2. **Copy relevant code (10-20 lines)**\n",
        "3. **Ask Claude**:\n",
        "\n",
        "```\n",
        "I'm building a launchable and got this error:\n",
        "\n",
        "[paste error]\n",
        "\n",
        "Here's the relevant code:\n",
        "\n",
        "[paste code]\n",
        "\n",
        "Context: I'm trying to [what you're doing]\n",
        "\n",
        "Can you:\n",
        "1. Explain what's wrong\n",
        "2. Provide the fix\n",
        "3. Suggest how to prevent this in the future\n",
        "```\n",
        "\n",
        "Claude is excellent at debugging because it can:\n",
        "- Understand full context\n",
        "- Suggest multiple solutions\n",
        "- Explain the underlying issue\n",
        "- Recommend best practices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 6: Debugging & Testing üîß\n",
        "\n",
        "## Common Launchable Errors\n",
        "\n",
        "### Error 1: \"CUDA out of memory\"\n",
        "\n",
        "**Cause**: Model/tensors too large for GPU\n",
        "\n",
        "**Solutions**:\n",
        "```python\n",
        "# 1. Clear cache regularly\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# 2. Use smaller batch sizes\n",
        "batch_size = 1  # Start small\n",
        "\n",
        "# 3. Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# 4. Use mixed precision\n",
        "from torch.cuda.amp import autocast\n",
        "with autocast():\n",
        "    output = model(input)\n",
        "\n",
        "# 5. Check memory before operations\n",
        "print(f\"Available: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "```\n",
        "\n",
        "### Error 2: \"RuntimeError: Expected all tensors to be on the same device\"\n",
        "\n",
        "**Cause**: Model on GPU, inputs on CPU (or vice versa)\n",
        "\n",
        "**Solution**:\n",
        "```python\n",
        "# Always move inputs to model's device\n",
        "device = next(model.parameters()).device\n",
        "inputs = inputs.to(device)\n",
        "```\n",
        "\n",
        "### Error 3: \"ModuleNotFoundError: No module named 'X'\"\n",
        "\n",
        "**Cause**: Missing dependency\n",
        "\n",
        "**Solutions**:\n",
        "```python\n",
        "# 1. Check requirements.txt is complete\n",
        "# 2. Install missing package\n",
        "!pip install package-name\n",
        "\n",
        "# 3. Verify installation\n",
        "import importlib\n",
        "importlib.import_module('package_name')\n",
        "```\n",
        "\n",
        "### Error 4: Model generates nonsense\n",
        "\n",
        "**Causes & Solutions**:\n",
        "- Temperature too high ‚Üí Lower to 0.7-0.9\n",
        "- No sampling ‚Üí Enable `do_sample=True`\n",
        "- Wrong tokenizer ‚Üí Verify tokenizer matches model\n",
        "- Not enough context ‚Üí Provide better prompt\n",
        "\n",
        "---\n",
        "\n",
        "## GPU-Specific Debugging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "GPU Debugging Toolkit\n",
        "Run this when something seems wrong with GPU\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "def gpu_health_check():\n",
        "    \"\"\"Comprehensive GPU health check\"\"\"\n",
        "    \n",
        "    print(\"=\" * 70)\n",
        "    print(\"üîß GPU HEALTH CHECK\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # 1. CUDA availability\n",
        "    print(\"\\n1Ô∏è‚É£ CUDA Availability\")\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    print(f\"   {'‚úÖ' if cuda_available else '‚ùå'} CUDA Available: {cuda_available}\")\n",
        "    \n",
        "    if not cuda_available:\n",
        "        print(\"\\n   ‚ö†Ô∏è  TROUBLESHOOTING:\")\n",
        "        print(\"   - Run 'nvidia-smi' in terminal\")\n",
        "        print(\"   - Check CUDA installation\")\n",
        "        print(\"   - Reinstall PyTorch with CUDA support\")\n",
        "        return\n",
        "    \n",
        "    # 2. GPU Details\n",
        "    print(\"\\n2Ô∏è‚É£ GPU Information\")\n",
        "    print(f\"   Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Compute Capability: {torch.cuda.get_device_capability(0)}\")\n",
        "    \n",
        "    # 3. Memory Status\n",
        "    print(\"\\n3Ô∏è‚É£ Memory Status\")\n",
        "    total_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    free = total_memory - reserved\n",
        "    \n",
        "    print(f\"   Total: {total_memory:.2f} GB\")\n",
        "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"   Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"   Free: {free:.2f} GB\")\n",
        "    \n",
        "    # Warning if low memory\n",
        "    if free < 1.0:\n",
        "        print(\"   ‚ö†Ô∏è  LOW MEMORY! Consider:\")\n",
        "        print(\"      - torch.cuda.empty_cache()\")\n",
        "        print(\"      - Reduce batch size\")\n",
        "        print(\"      - Use smaller model\")\n",
        "    \n",
        "    # 4. Test GPU computation\n",
        "    print(\"\\n4Ô∏è‚É£ GPU Computation Test\")\n",
        "    try:\n",
        "        test = torch.randn(1000, 1000, device='cuda')\n",
        "        result = torch.matmul(test, test)\n",
        "        print(f\"   ‚úÖ Computation successful\")\n",
        "        print(f\"   ‚úÖ Result shape: {result.shape}\")\n",
        "        print(f\"   ‚úÖ Result device: {result.device}\")\n",
        "        del test, result\n",
        "        torch.cuda.empty_cache()\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Computation failed: {e}\")\n",
        "    \n",
        "    # 5. Check nvidia-smi\n",
        "    print(\"\\n5Ô∏è‚É£ nvidia-smi Status\")\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total',\n",
        "                               '--format=csv,noheader,nounits'],\n",
        "                              capture_output=True, text=True, timeout=5)\n",
        "        if result.returncode == 0:\n",
        "            gpu_util, mem_used, mem_total = result.stdout.strip().split(',')\n",
        "            print(f\"   GPU Utilization: {gpu_util.strip()}%\")\n",
        "            print(f\"   Memory Used: {mem_used.strip()} MB / {mem_total.strip()} MB\")\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  nvidia-smi not available\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ö†Ô∏è  Could not run nvidia-smi: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ Health check complete!\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Run the health check\n",
        "gpu_health_check()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing Checklist\n",
        "\n",
        "Before you share your launchable, test EVERYTHING:\n",
        "\n",
        "### ‚úÖ Pre-Launch Checklist\n",
        "\n",
        "**Environment Testing:**\n",
        "- [ ] Restart kernel and run all cells sequentially\n",
        "- [ ] All cells complete without errors\n",
        "- [ ] GPU is actually being used (check nvidia-smi)\n",
        "- [ ] Memory usage is reasonable (<80% GPU memory)\n",
        "- [ ] All imports work\n",
        "\n",
        "**Code Quality:**\n",
        "- [ ] No hardcoded paths or personal info\n",
        "- [ ] All variables are defined before use\n",
        "- [ ] Error messages are helpful\n",
        "- [ ] Progress indicators for long operations\n",
        "- [ ] GPU verification in first executable cell\n",
        "\n",
        "**Documentation:**\n",
        "- [ ] README.md is clear and complete\n",
        "- [ ] requirements.txt has all dependencies\n",
        "- [ ] .gitignore excludes large files\n",
        "- [ ] Code has explanatory comments\n",
        "- [ ] Examples work as described\n",
        "\n",
        "**User Experience:**\n",
        "- [ ] Instructions are easy to follow\n",
        "- [ ] Exercises have solutions or hints\n",
        "- [ ] Output is formatted and readable\n",
        "- [ ] No broken links\n",
        "- [ ] Appropriate emojis and formatting\n",
        "\n",
        "**Final Test:**\n",
        "- [ ] Fresh environment test (new venv)\n",
        "- [ ] Test on minimum GPU requirements\n",
        "- [ ] Ask someone else to try it\n",
        "- [ ] Fix any issues they encounter\n",
        "\n",
        "---\n",
        "\n",
        "## Automated Testing Pattern\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 7: Git & GitHub Setup üì¶\n",
        "\n",
        "## Version Control for Launchables\n",
        "\n",
        "Every launchable should be in version control. Here's the workflow:\n",
        "\n",
        "## Initial Setup\n",
        "\n",
        "### 1. Initialize Git Repository\n",
        "\n",
        "```bash\n",
        "# Navigate to your launchable directory\n",
        "cd /path/to/your-launchable\n",
        "\n",
        "# Initialize git\n",
        "git init\n",
        "\n",
        "# Add .gitignore (prevents committing large files)\n",
        "# Your .gitignore should already exist!\n",
        "```\n",
        "\n",
        "### 2. Verify SSH Authentication\n",
        "\n",
        "Before pushing to GitHub, verify SSH is set up:\n",
        "\n",
        "```bash\n",
        "# Test GitHub SSH connection\n",
        "ssh -T git@github.com\n",
        "\n",
        "# Expected output:\n",
        "# \"Hi username! You've successfully authenticated...\"\n",
        "```\n",
        "\n",
        "If this fails:\n",
        "- Generate SSH key: `ssh-keygen -t ed25519 -C \"your_email@example.com\"`\n",
        "- Add to GitHub: Settings ‚Üí SSH and GPG keys ‚Üí New SSH key\n",
        "- Guide: https://docs.github.com/en/authentication/connecting-to-github-with-ssh\n",
        "\n",
        "### 3. Create GitHub Repository\n",
        "\n",
        "1. Go to https://github.com/new\n",
        "2. Name it (e.g., `my-awesome-launchable`)\n",
        "3. **Don't** initialize with README (we have one!)\n",
        "4. Click \"Create repository\"\n",
        "\n",
        "---\n",
        "\n",
        "## Git Workflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Standard Git Commands for Launchables\n",
        "\n",
        "```bash\n",
        "# 1. Check status (what's changed?)\n",
        "git status\n",
        "\n",
        "# 2. Add all files\n",
        "git add .\n",
        "\n",
        "# Or add specific files\n",
        "git add README.md requirements.txt your-notebook.ipynb\n",
        "\n",
        "# 3. Commit with descriptive message\n",
        "git commit -m \"Initial commit: Working launchable with GPU verification\"\n",
        "\n",
        "# 4. Add remote (from GitHub's instructions)\n",
        "git remote add origin git@github.com:YOUR-USERNAME/YOUR-REPO.git\n",
        "\n",
        "# 5. Push to GitHub\n",
        "git branch -M main\n",
        "git push -u origin main\n",
        "\n",
        "# For subsequent updates:\n",
        "git add .\n",
        "git commit -m \"Update: Add interactive examples\"\n",
        "git push\n",
        "```\n",
        "\n",
        "## What to Commit vs Ignore\n",
        "\n",
        "### ‚úÖ DO Commit:\n",
        "- Source code (`.ipynb`, `.py`)\n",
        "- Documentation (`.md` files)\n",
        "- Configuration (`requirements.txt`, `.gitignore`)\n",
        "- Small assets (images < 1MB)\n",
        "\n",
        "### ‚ùå DON'T Commit:\n",
        "- Model weights (`.bin`, `.safetensors`, `.pth`)\n",
        "- Virtual environments (`venv/`, `env/`)\n",
        "- Cache files (`__pycache__/`, `.ipynb_checkpoints/`)\n",
        "- Large datasets\n",
        "- API keys or secrets\n",
        "\n",
        "**Your `.gitignore` handles this automatically!**\n",
        "\n",
        "---\n",
        "\n",
        "## Pro Git Tips\n",
        "\n",
        "### Commit Message Best Practices\n",
        "\n",
        "```bash\n",
        "# Good messages\n",
        "git commit -m \"Add GPU memory optimization\"\n",
        "git commit -m \"Fix: Handle CUDA out of memory error\"\n",
        "git commit -m \"Update README with prerequisites\"\n",
        "\n",
        "# Bad messages\n",
        "git commit -m \"update\"\n",
        "git commit -m \"fix stuff\"\n",
        "git commit -m \"asdf\"\n",
        "```\n",
        "\n",
        "### Useful Git Commands\n",
        "\n",
        "```bash\n",
        "# See commit history\n",
        "git log --oneline\n",
        "\n",
        "# Undo last commit (keep changes)\n",
        "git reset --soft HEAD~1\n",
        "\n",
        "# Discard local changes\n",
        "git checkout -- filename\n",
        "\n",
        "# Create a branch for experiments\n",
        "git checkout -b experiment\n",
        "\n",
        "# View what changed\n",
        "git diff\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 8: Deploying to Brev üöÄ\n",
        "\n",
        "## Why Brev?\n",
        "\n",
        "**Brev** makes GPU deployment effortless:\n",
        "- üöÄ One-click deployment from GitHub\n",
        "- üíª Instant GPU access\n",
        "- üåê Shareable links\n",
        "- üí∞ Pay only for what you use\n",
        "- üîÑ Easy updates\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. **GitHub Repository** - Your launchable pushed to GitHub ‚úÖ\n",
        "2. **Brev Account** - Sign up at [brev.dev](https://brev.dev)\n",
        "3. **Tested Notebook** - Everything works locally ‚úÖ\n",
        "\n",
        "---\n",
        "\n",
        "## Deployment Checklist\n",
        "\n",
        "### Before Deploying\n",
        "\n",
        "- [ ] **Test locally**: Restart kernel, run all cells successfully\n",
        "- [ ] **GPU verification**: First cell checks GPU\n",
        "- [ ] **Dependencies**: `requirements.txt` is complete\n",
        "- [ ] **Documentation**: README.md is clear\n",
        "- [ ] **No secrets**: No API keys or passwords in code\n",
        "- [ ] **Clean repo**: `.gitignore` excludes unnecessary files\n",
        "- [ ] **Pushed to GitHub**: Latest version on main branch\n",
        "\n",
        "### Deployment Steps\n",
        "\n",
        "1. **Go to Brev.dev**\n",
        "   - Log in with GitHub\n",
        "\n",
        "2. **Create New Instance**\n",
        "   - Click \"New Instance\"\n",
        "   - Select GPU type (start with T4 or A10)\n",
        "\n",
        "3. **Connect GitHub Repository**\n",
        "   - Authorize Brev to access your repos\n",
        "   - Select your launchable repository\n",
        "\n",
        "4. **Configure Environment**\n",
        "   - Brev auto-detects `requirements.txt`\n",
        "   - Selects Python version\n",
        "   - Installs dependencies\n",
        "\n",
        "5. **Launch!**\n",
        "   - Instance boots in 1-2 minutes\n",
        "   - Jupyter opens automatically\n",
        "   - GPU ready to use\n",
        "\n",
        "6. **Share**\n",
        "   - Get shareable link\n",
        "   - Anyone can access and run your launchable\n",
        "   - No setup required for users!\n",
        "\n",
        "---\n",
        "\n",
        "## Brev Configuration Tips\n",
        "\n",
        "### Optimize for Fast Boot\n",
        "\n",
        "```txt\n",
        "# requirements.txt - Pin versions for reproducibility\n",
        "torch==2.1.0\n",
        "transformers==4.35.0\n",
        "# ... rest of your deps\n",
        "```\n",
        "\n",
        "### Add a `brev.yaml` (Optional)\n",
        "\n",
        "```yaml\n",
        "# brev.yaml - Advanced configuration\n",
        "python_version: \"3.10\"\n",
        "gpu: \"t4\"\n",
        "environment:\n",
        "  TRANSFORMERS_CACHE: \"/workspace/.cache\"\n",
        "```\n",
        "\n",
        "### Post-Deployment Testing\n",
        "\n",
        "Once deployed:\n",
        "1. Open the Brev link\n",
        "2. Run through the entire notebook\n",
        "3. Verify GPU works\n",
        "4. Test all interactive elements\n",
        "5. Check that outputs are correct\n",
        "\n",
        "---\n",
        "\n",
        "## Updating Your Launchable\n",
        "\n",
        "When you make changes:\n",
        "\n",
        "```bash\n",
        "# Local: Make improvements\n",
        "# Edit notebook, test thoroughly\n",
        "\n",
        "# Commit and push\n",
        "git add .\n",
        "git commit -m \"Update: Improve error handling\"\n",
        "git push\n",
        "\n",
        "# Brev: Restart instance\n",
        "# Changes automatically pulled on restart\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Monitoring and Debugging on Brev\n",
        "\n",
        "### Check GPU Usage\n",
        "\n",
        "```bash\n",
        "# In Brev terminal\n",
        "watch -n 1 nvidia-smi\n",
        "```\n",
        "\n",
        "### View Logs\n",
        "\n",
        "```bash\n",
        "# See installation logs\n",
        "cat /workspace/.brev/logs/install.log\n",
        "\n",
        "# Python errors\n",
        "# Visible in Jupyter notebook output\n",
        "```\n",
        "\n",
        "### Common Brev Issues\n",
        "\n",
        "**Issue**: Dependencies fail to install\n",
        "- **Fix**: Check `requirements.txt` syntax\n",
        "- **Fix**: Pin versions explicitly\n",
        "\n",
        "**Issue**: Notebook cells fail on Brev but work locally\n",
        "- **Fix**: Different GPU type - adjust memory usage\n",
        "- **Fix**: Check CUDA version compatibility\n",
        "\n",
        "**Issue**: Slow boot time\n",
        "- **Fix**: Minimize dependencies\n",
        "- **Fix**: Use smaller base image\n",
        "\n",
        "---\n",
        "\n",
        "## Production Best Practices\n",
        "\n",
        "### Resource Management\n",
        "\n",
        "```python\n",
        "# Clean up after intensive operations\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def cleanup():\n",
        "    \"\"\"Call this after heavy GPU operations\"\"\"\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "# Use it\n",
        "# ... heavy operation ...\n",
        "cleanup()\n",
        "```\n",
        "\n",
        "### Error Handling\n",
        "\n",
        "```python\n",
        "try:\n",
        "    # Your GPU code\n",
        "    model = load_model()\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"üí° Try: Restart kernel or use smaller batch size\")\n",
        "    raise\n",
        "```\n",
        "\n",
        "### User Guidance\n",
        "\n",
        "Always include:\n",
        "- Expected runtime for each cell\n",
        "- What outputs users should see\n",
        "- What to do if errors occur\n",
        "- How to get help\n",
        "\n",
        "---\n",
        "\n",
        "## Resources\n",
        "\n",
        "- **Brev Documentation**: [docs.brev.dev](https://docs.brev.dev)\n",
        "- **Brev Discord**: Community support\n",
        "- **GPU Pricing**: [brev.dev/pricing](https://brev.dev/pricing)\n",
        "- **Example Deployments**: [github.com/brevdev/launchables](https://github.com/brevdev/launchables)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 9: Your First Launchable Exercise üéì\n",
        "\n",
        "## Hands-On Project: Build a Simple Sentiment Analyzer\n",
        "\n",
        "**Goal**: Create a working GPU-accelerated sentiment analysis launchable\n",
        "\n",
        "**Time**: 15-20 minutes\n",
        "\n",
        "**What You'll Build**:\n",
        "- Load a pre-trained sentiment model (DistilBERT)\n",
        "- Create an interactive analyzer\n",
        "- Demonstrate GPU acceleration\n",
        "- Show proper device management\n",
        "\n",
        "This is a **complete mini-launchable** that you can expand and customize!\n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Load Sentiment Analysis Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 1: Load a pre-trained sentiment analysis model\n",
        "This demonstrates best practices for model loading\n",
        "\"\"\"\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üéØ SENTIMENT ANALYSIS LAUNCHABLE\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nüì¶ Loading Model...\\n\")\n",
        "\n",
        "# Device setup (ALWAYS first!)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üéØ Device: {device}\")\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "print(f\"üì• Loading {model_name}...\")\n",
        "\n",
        "sentiment_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "sentiment_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU\n",
        "print(f\"\\nüîÑ Moving model to {device}...\")\n",
        "sentiment_model = sentiment_model.to(device)\n",
        "sentiment_model.eval()  # Set to evaluation mode\n",
        "\n",
        "# Verify\n",
        "model_device = next(sentiment_model.parameters()).device\n",
        "print(f\"‚úÖ Model loaded on: {model_device}\")\n",
        "\n",
        "# Check memory\n",
        "if device.type == \"cuda\":\n",
        "    memory = torch.cuda.memory_allocated(0) / 1e6\n",
        "    print(f\"üíæ GPU Memory: {memory:.2f} MB\")\n",
        "\n",
        "print(\"\\n‚úÖ Setup complete! Ready for sentiment analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Sentiment Analysis Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 2: Create reusable sentiment analysis function\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def analyze_sentiment(text, show_details=True):\n",
        "    \"\"\"\n",
        "    Analyze sentiment of text using GPU-accelerated model\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "        show_details: Whether to print detailed info\n",
        "        \n",
        "    Returns:\n",
        "        dict: Contains sentiment, confidence, and label\n",
        "    \"\"\"\n",
        "    \n",
        "    if show_details:\n",
        "        print(f\"üìù Analyzing: '{text}'\\n\")\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = sentiment_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=True\n",
        "    )\n",
        "    \n",
        "    # Move to device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    if show_details:\n",
        "        print(f\"üéØ Inputs on: {inputs['input_ids'].device}\")\n",
        "    \n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        outputs = sentiment_model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "    # Get probabilities\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    confidence, predicted_class = torch.max(probs, dim=1)\n",
        "    \n",
        "    # Map to labels\n",
        "    labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
        "    sentiment = labels[predicted_class.item()]\n",
        "    confidence_score = confidence.item()\n",
        "    \n",
        "    # Display results\n",
        "    if show_details:\n",
        "        print(f\"   Logits device: {logits.device}\")\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"Result: {sentiment}\")\n",
        "        print(f\"Confidence: {confidence_score:.2%}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        \n",
        "        # Show both probabilities\n",
        "        print(\"Full Distribution:\")\n",
        "        for label, prob in zip(labels, probs[0]):\n",
        "            bar = \"‚ñà\" * int(prob.item() * 50)\n",
        "            print(f\"  {label:>8}: {prob.item():.2%} {bar}\")\n",
        "        print()\n",
        "    \n",
        "    return {\n",
        "        \"sentiment\": sentiment,\n",
        "        \"confidence\": confidence_score,\n",
        "        \"label\": predicted_class.item(),\n",
        "        \"probabilities\": probs[0].cpu().tolist()\n",
        "    }\n",
        "\n",
        "# Test it!\n",
        "print(\"üß™ Testing sentiment analyzer...\\n\")\n",
        "result = analyze_sentiment(\"This tutorial is amazing! I love learning about GPUs!\")\n",
        "print(\"‚úÖ Test complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Interactive Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 3: Batch analysis with progress tracking\n",
        "\"\"\"\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üìä BATCH SENTIMENT ANALYSIS\")\n",
        "print(\"=\" * 70)\n",
        "print()\n",
        "\n",
        "# Sample texts to analyze\n",
        "texts_to_analyze = [\n",
        "    \"NVIDIA GPUs are incredibly powerful for AI workloads!\",\n",
        "    \"I'm frustrated with how long this is taking.\",\n",
        "    \"The weather today is okay, nothing special.\",\n",
        "    \"This is the worst experience I've ever had.\",\n",
        "    \"Absolutely fantastic! Best decision ever.\",\n",
        "    \"Launchables make AI development so much easier!\",\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(\"üîÑ Analyzing multiple texts...\\n\")\n",
        "\n",
        "for i, text in enumerate(tqdm(texts_to_analyze, desc=\"Processing\"), 1):\n",
        "    result = analyze_sentiment(text, show_details=False)\n",
        "    results.append({\n",
        "        \"text\": text,\n",
        "        **result\n",
        "    })\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    sentiment = result['sentiment']\n",
        "    confidence = result['confidence']\n",
        "    text = result['text']\n",
        "    \n",
        "    # Emoji based on sentiment\n",
        "    emoji = \"üòä\" if sentiment == \"POSITIVE\" else \"üòû\"\n",
        "    \n",
        "    print(f\"\\n{i}. {emoji} {sentiment} ({confidence:.1%})\")\n",
        "    print(f\"   \\\"{text}\\\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"‚úÖ Analyzed {len(results)} texts using GPU acceleration!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéÆ Your Turn!\n",
        "\n",
        "**Exercise**: Modify the sentiment analyzer above:\n",
        "\n",
        "1. **Try your own text**: Change the texts in the batch analysis\n",
        "2. **Add more examples**: Expand the list with your own sentences\n",
        "3. **Visualize results**: Add a simple bar chart with matplotlib\n",
        "4. **Compare performance**: Time CPU vs GPU execution\n",
        "\n",
        "**Challenge**: Turn this into a full launchable!\n",
        "1. Create a new directory `sentiment-analysis-launchable/`\n",
        "2. Move this code to a new notebook\n",
        "3. Add README.md and requirements.txt\n",
        "4. Push to GitHub\n",
        "5. Deploy to Brev!\n",
        "\n",
        "---\n",
        "\n",
        "## What You Just Learned\n",
        "\n",
        "‚úÖ **Device Management** - Proper GPU setup  \n",
        "‚úÖ **Model Loading** - Pre-trained models on GPU  \n",
        "‚úÖ **Inference** - GPU-accelerated predictions  \n",
        "‚úÖ **Batch Processing** - Efficient multi-input handling  \n",
        "‚úÖ **Progress Tracking** - User-friendly feedback  \n",
        "‚úÖ **Results Display** - Clear, formatted output  \n",
        "\n",
        "**This is a complete launchable pattern!** You can use this as a template for any transformer-based model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Section 10: Resources & Next Steps üåü\n",
        "\n",
        "## Congratulations! üéâ\n",
        "\n",
        "You've completed the **How to Build NVIDIA Launchables** tutorial!\n",
        "\n",
        "You now know:\n",
        "- ‚úÖ The launchables pattern and structure\n",
        "- ‚úÖ GPU-first development practices\n",
        "- ‚úÖ How to build interactive demos\n",
        "- ‚úÖ AI-assisted development with Claude + Cursor\n",
        "- ‚úÖ Debugging and testing strategies\n",
        "- ‚úÖ Git workflow for launchables\n",
        "- ‚úÖ Deploying to Brev\n",
        "- ‚úÖ Building real GPU-accelerated applications\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Essential Resources\n",
        "\n",
        "### Official Documentation\n",
        "- **Launchables Repository**: [github.com/brevdev/launchables](https://github.com/brevdev/launchables)\n",
        "- **Brev Documentation**: [docs.brev.dev](https://docs.brev.dev)\n",
        "- **NVIDIA CUDA**: [developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit)\n",
        "- **PyTorch**: [pytorch.org/docs](https://pytorch.org/docs)\n",
        "- **Transformers**: [huggingface.co/docs/transformers](https://huggingface.co/docs/transformers)\n",
        "\n",
        "### Learning Resources\n",
        "- **GPU Programming**: [NVIDIA Deep Learning Institute](https://www.nvidia.com/en-us/training/)\n",
        "- **Transformers Course**: [huggingface.co/course](https://huggingface.co/course)\n",
        "- **MLOps Best Practices**: [ml-ops.org](https://ml-ops.org/)\n",
        "\n",
        "### Community\n",
        "- **Brev Discord**: Get help and share your launchables\n",
        "- **Launchables Discussions**: GitHub Discussions on brevdev/launchables\n",
        "- **HuggingFace Forums**: [discuss.huggingface.co](https://discuss.huggingface.co)\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Launchable Ideas to Build\n",
        "\n",
        "Need inspiration? Here are ideas across different skill levels:\n",
        "\n",
        "### Beginner-Friendly\n",
        "1. **Image Classification** - Fine-tune ResNet on custom dataset\n",
        "2. **Text Summarization** - Summarize articles with T5\n",
        "3. **Question Answering** - Build a Q&A system with BERT\n",
        "4. **Style Transfer** - Artistic style transfer with CNNs\n",
        "5. **Named Entity Recognition** - Extract entities from text\n",
        "\n",
        "### Intermediate\n",
        "1. **Fine-tune LLaMA 2** - LoRA fine-tuning on custom data\n",
        "2. **RAG System** - Retrieval-augmented generation from scratch\n",
        "3. **Stable Diffusion Fine-tuning** - Train on your images\n",
        "4. **Multi-modal Search** - CLIP-based image-text search\n",
        "5. **Voice Cloning** - Text-to-speech with custom voices\n",
        "\n",
        "### Advanced\n",
        "1. **LLM Inference Optimization** - vLLM, TensorRT-LLM comparison\n",
        "2. **Distributed Training** - Multi-GPU fine-tuning\n",
        "3. **Custom CUDA Kernels** - Optimize specific operations\n",
        "4. **Model Quantization** - INT8/INT4 optimization guide\n",
        "5. **Production ML System** - End-to-end deployment pipeline\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Your Next Steps\n",
        "\n",
        "### 1. Build Your First Launchable (Today!)\n",
        "- Pick an idea from above (or your own)\n",
        "- Use Claude to generate a detailed Cursor prompt\n",
        "- Implement it following this tutorial's pattern\n",
        "- Test thoroughly\n",
        "- Push to GitHub\n",
        "- Deploy to Brev\n",
        "- Share it!\n",
        "\n",
        "### 2. Join the Community (This Week)\n",
        "- Star the [brevdev/launchables](https://github.com/brevdev/launchables) repo\n",
        "- Join Brev Discord\n",
        "- Share your first launchable\n",
        "- Get feedback and iterate\n",
        "\n",
        "### 3. Keep Learning (Ongoing)\n",
        "- Build one launchable per month\n",
        "- Experiment with new models and techniques\n",
        "- Read other people's launchables\n",
        "- Contribute improvements\n",
        "- Help beginners get started\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Call to Action\n",
        "\n",
        "**The launchables ecosystem needs YOU!**\n",
        "\n",
        "The best way to learn is by building and sharing. Your launchable might:\n",
        "- Help someone learn a new skill\n",
        "- Showcase a novel technique\n",
        "- Solve a real problem\n",
        "- Inspire others to build\n",
        "\n",
        "**Jensen's vision is to democratize AI development.** Every launchable you create makes AI more accessible.\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Quick Start Template\n",
        "\n",
        "Ready to start? Copy this to Claude to begin your next launchable:\n",
        "\n",
        "```\n",
        "Create a NVIDIA Launchable for [YOUR IDEA].\n",
        "\n",
        "Structure:\n",
        "- Jupyter notebook in root directory\n",
        "- Include requirements.txt, README.md, .gitignore\n",
        "- GPU verification as first executable cell\n",
        "- 5-7 interactive sections\n",
        "- Working examples with real models\n",
        "- Progress indicators and error handling\n",
        "- Test on 8GB GPU\n",
        "\n",
        "Make it beginner-friendly, well-documented, and deployable to Brev.\n",
        "Follow the patterns from the \"How to Build Launchables\" tutorial.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üôè Thank You!\n",
        "\n",
        "Thank you for completing this tutorial! You're now part of the launchables community.\n",
        "\n",
        "Remember:\n",
        "- **Start simple** - Your first launchable doesn't need to be perfect\n",
        "- **Ship fast** - Get it working, then improve\n",
        "- **Share openly** - Others will learn from your work\n",
        "- **Ask for help** - Community is here to support you\n",
        "- **Pay it forward** - Help the next person getting started\n",
        "\n",
        "---\n",
        "\n",
        "## ‚≠ê One More Thing...\n",
        "\n",
        "If this tutorial helped you, please:\n",
        "1. ‚≠ê Star the repository\n",
        "2. üê¶ Share on social media\n",
        "3. üí¨ Tell your friends\n",
        "4. üèóÔ∏è Build something amazing\n",
        "\n",
        "**Now go build your launchable! The world is waiting to see what you create.** üöÄ\n",
        "\n",
        "---\n",
        "\n",
        "*Tutorial created with ‚ù§Ô∏è for the NVIDIA Launchables community*\n",
        "\n",
        "*\"The future of AI is being built by people like you, one launchable at a time.\"*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Quick test suite for your launchable\n",
        "Run this before sharing!\n",
        "\"\"\"\n",
        "\n",
        "def test_launchable():\n",
        "    \"\"\"Run automated tests\"\"\"\n",
        "    \n",
        "    print(\"üß™ Running Launchable Tests...\\n\")\n",
        "    \n",
        "    tests_passed = 0\n",
        "    tests_total = 0\n",
        "    \n",
        "    # Test 1: GPU Available\n",
        "    tests_total += 1\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"‚úÖ Test 1: GPU available\")\n",
        "        tests_passed += 1\n",
        "    else:\n",
        "        print(\"‚ùå Test 1: GPU not available\")\n",
        "    \n",
        "    # Test 2: Model loaded\n",
        "    tests_total += 1\n",
        "    try:\n",
        "        if model is not None and hasattr(model, 'generate'):\n",
        "            print(\"‚úÖ Test 2: Model loaded correctly\")\n",
        "            tests_passed += 1\n",
        "        else:\n",
        "            print(\"‚ùå Test 2: Model not loaded\")\n",
        "    except NameError:\n",
        "        print(\"‚ùå Test 2: Model not defined\")\n",
        "    \n",
        "    # Test 3: Model on GPU\n",
        "    tests_total += 1\n",
        "    try:\n",
        "        model_device = next(model.parameters()).device\n",
        "        if model_device.type == 'cuda':\n",
        "            print(f\"‚úÖ Test 3: Model on GPU ({model_device})\")\n",
        "            tests_passed += 1\n",
        "        else:\n",
        "            print(f\"‚ùå Test 3: Model not on GPU (on {model_device})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Test 3: Could not check model device - {e}\")\n",
        "    \n",
        "    # Test 4: Tokenizer loaded\n",
        "    tests_total += 1\n",
        "    try:\n",
        "        if tokenizer is not None:\n",
        "            test_text = tokenizer(\"test\")\n",
        "            print(\"‚úÖ Test 4: Tokenizer working\")\n",
        "            tests_passed += 1\n",
        "        else:\n",
        "            print(\"‚ùå Test 4: Tokenizer not loaded\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Test 4: Tokenizer error - {e}\")\n",
        "    \n",
        "    # Test 5: Generation works\n",
        "    tests_total += 1\n",
        "    try:\n",
        "        test_input = tokenizer(\"Test\", return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            test_output = model.generate(test_input['input_ids'], max_length=10)\n",
        "        print(\"‚úÖ Test 5: Text generation working\")\n",
        "        tests_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Test 5: Generation failed - {e}\")\n",
        "    \n",
        "    # Test 6: Memory management\n",
        "    tests_total += 1\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        usage_percent = (allocated / total) * 100\n",
        "        \n",
        "        if usage_percent < 90:\n",
        "            print(f\"‚úÖ Test 6: Memory usage OK ({usage_percent:.1f}%)\")\n",
        "            tests_passed += 1\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Test 6: High memory usage ({usage_percent:.1f}%)\")\n",
        "    \n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"üìä Test Results: {tests_passed}/{tests_total} passed\")\n",
        "    \n",
        "    if tests_passed == tests_total:\n",
        "        print(\"üéâ All tests passed! Your launchable is ready!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Some tests failed. Review and fix before sharing.\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# Run tests\n",
        "test_launchable()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

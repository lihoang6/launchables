{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrating Nemotron Nano 9B v2 with NVIDIA NIM\n",
        "\n",
        "This notebook demonstrates how to integrate the Nemotron Nano 9B v2 model with NVIDIA NIM for production deployments.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- NVIDIA API key from [build.nvidia.com](https://build.nvidia.com/)\n",
        "- Python 3.8+\n",
        "- OpenAI Python library\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup API Key\n",
        "\n",
        "Get your API key from [build.nvidia.com](https://build.nvidia.com/) and set it as an environment variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your API key\n",
        "NVIDIA_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual key\n",
        "# Or use environment variable: os.getenv(\"NVIDIA_API_KEY\")\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=NVIDIA_API_KEY\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Chat Completion\n",
        "\n",
        "Simple conversational interaction with Nemotron Nano 9B v2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Streaming Responses\n",
        "\n",
        "Stream tokens as they are generated for a better user experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a Python function to calculate Fibonacci numbers.\"}\n",
        "    ],\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Function Calling\n",
        "\n",
        "Demonstrate intelligent function calling capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define available functions\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"City name, e.g. San Francisco\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like in Tokyo?\"}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "\n",
        "# Check if the model wants to call a function\n",
        "message = response.choices[0].message\n",
        "if message.tool_calls:\n",
        "    print(\"Function call requested:\")\n",
        "    print(f\"Function: {message.tool_calls[0].function.name}\")\n",
        "    print(f\"Arguments: {message.tool_calls[0].function.arguments}\")\n",
        "else:\n",
        "    print(message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Multi-turn Conversation\n",
        "\n",
        "Build context over multiple exchanges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful coding tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I'm learning Python. What are decorators?\"}\n",
        "]\n",
        "\n",
        "# First response\n",
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=conversation,\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "assistant_msg = response.choices[0].message.content\n",
        "conversation.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "print(f\"Assistant: {assistant_msg}\\n\")\n",
        "\n",
        "# Follow-up question\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Can you show me a practical example?\"})\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=conversation,\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "print(f\"Assistant: {response.choices[0].message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Integration with Web Demo\n",
        "\n",
        "To integrate this notebook's code with the web demo:\n",
        "\n",
        "1. Create a backend API server (Flask, FastAPI, etc.)\n",
        "2. Replace mock responses in `src/index.html` with API calls\n",
        "3. Implement streaming using Server-Sent Events (SSE)\n",
        "4. Add proper error handling and rate limiting\n",
        "\n",
        "Example backend structure:\n",
        "\n",
        "```python\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import StreamingResponse\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/api/chat\")\n",
        "async def chat(request: ChatRequest):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "        messages=request.messages,\n",
        "        stream=True\n",
        "    )\n",
        "    \n",
        "    async def generate():\n",
        "        for chunk in response:\n",
        "            if chunk.choices[0].delta.content:\n",
        "                yield f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n",
        "    \n",
        "    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n",
        "```\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [NVIDIA NIM API Documentation](https://docs.nvidia.com/nim/)\n",
        "- [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Building with NVIDIA Nemotron!** ðŸš€\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Integrating Nemotron Nano 9B v2 with NVIDIA NIM\n",
        "\n",
        "This notebook demonstrates how to integrate the Nemotron Nano 9B v2 model with NVIDIA NIM for production deployments.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- NVIDIA API key from [build.nvidia.com](https://build.nvidia.com/)\n",
        "- Python 3.8+\n",
        "- OpenAI Python library\n",
        "\n",
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages with error handling\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_package(package_name):\n",
        "    \"\"\"Install a package handling various environment scenarios\"\"\"\n",
        "    try:\n",
        "        # Try using pip module\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"‚úÖ Successfully installed {package_name} using pip\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        try:\n",
        "            # Try ensurepip to bootstrap pip first\n",
        "            print(\"‚ö†Ô∏è pip not found, attempting to install pip...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"ensurepip\", \"--default-pip\"])\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "            print(f\"‚úÖ Successfully installed pip and {package_name}\")\n",
        "        except:\n",
        "            try:\n",
        "                # Try conda as fallback\n",
        "                subprocess.check_call([\"conda\", \"install\", \"-y\", package_name])\n",
        "                print(f\"‚úÖ Successfully installed {package_name} using conda\")\n",
        "            except:\n",
        "                print(f\"‚ùå Failed to install {package_name}\")\n",
        "                print(f\"Please run manually: pip install {package_name}\")\n",
        "                raise\n",
        "\n",
        "# Install openai\n",
        "install_package(\"openai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup API Key\n",
        "\n",
        "Get your API key from [build.nvidia.com](https://build.nvidia.com/) and set it as an environment variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your API key\n",
        "NVIDIA_API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual key\n",
        "# Or use environment variable: os.getenv(\"NVIDIA_API_KEY\")\n",
        "\n",
        "# Validate API key\n",
        "if NVIDIA_API_KEY == \"YOUR_API_KEY_HERE\" or not NVIDIA_API_KEY:\n",
        "    raise ValueError(\n",
        "        \"‚ùå ERROR: Please set your NVIDIA API key!\\n\\n\"\n",
        "        \"Steps:\\n\"\n",
        "        \"1. Go to https://build.nvidia.com/\\n\"\n",
        "        \"2. Sign in and generate an API key\\n\"\n",
        "        \"3. Replace 'YOUR_API_KEY_HERE' above with your actual key\\n\"\n",
        "        \"4. Re-run this cell\\n\\n\"\n",
        "        \"Example: NVIDIA_API_KEY = 'nvapi-xxxxxxxxxxxxx'\"\n",
        "    )\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=NVIDIA_API_KEY\n",
        ")\n",
        "\n",
        "print(\"‚úÖ NVIDIA NIM client initialized!\")\n",
        "print(f\"üîë API key: {NVIDIA_API_KEY[:10]}...{NVIDIA_API_KEY[-4:]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic Chat Completion\n",
        "\n",
        "Simple conversational interaction with Nemotron Nano 9B v2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Streaming Responses\n",
        "\n",
        "Stream tokens as they are generated for a better user experience.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Write a Python function to calculate Fibonacci numbers.\"}\n",
        "    ],\n",
        "    temperature=0.5,\n",
        "    max_tokens=1024,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"Streaming response:\")\n",
        "for chunk in response:\n",
        "    if chunk.choices[0].delta.content:\n",
        "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
        "print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Function Calling\n",
        "\n",
        "Demonstrate intelligent function calling capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define available functions\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_weather\",\n",
        "            \"description\": \"Get current weather for a location\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"City name, e.g. San Francisco\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"What's the weather like in Tokyo?\"}\n",
        "    ],\n",
        "    tools=tools,\n",
        "    tool_choice=\"auto\"\n",
        ")\n",
        "\n",
        "# Check if the model wants to call a function\n",
        "message = response.choices[0].message\n",
        "if message.tool_calls:\n",
        "    print(\"Function call requested:\")\n",
        "    print(f\"Function: {message.tool_calls[0].function.name}\")\n",
        "    print(f\"Arguments: {message.tool_calls[0].function.arguments}\")\n",
        "else:\n",
        "    print(message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Multi-turn Conversation\n",
        "\n",
        "Build context over multiple exchanges.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful coding tutor.\"},\n",
        "    {\"role\": \"user\", \"content\": \"I'm learning Python. What are decorators?\"}\n",
        "]\n",
        "\n",
        "# First response\n",
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=conversation,\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "assistant_msg = response.choices[0].message.content\n",
        "conversation.append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "print(f\"Assistant: {assistant_msg}\\n\")\n",
        "\n",
        "# Follow-up question\n",
        "conversation.append({\"role\": \"user\", \"content\": \"Can you show me a practical example?\"})\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "    messages=conversation,\n",
        "    temperature=0.7,\n",
        "    max_tokens=512\n",
        ")\n",
        "\n",
        "print(f\"Assistant: {response.choices[0].message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Integration Example\n",
        "\n",
        "Build a production API server to serve Nemotron Nano 9B v2:\n",
        "\n",
        "### FastAPI Backend Example\n",
        "\n",
        "```python\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import StreamingResponse\n",
        "from pydantic import BaseModel\n",
        "from openai import OpenAI\n",
        "import os\n",
        "\n",
        "app = FastAPI(title=\"Nemotron Nano API\")\n",
        "\n",
        "# Initialize client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
        "    api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
        ")\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    messages: list\n",
        "    stream: bool = True\n",
        "    temperature: float = 0.7\n",
        "    max_tokens: int = 1024\n",
        "\n",
        "@app.post(\"/api/chat\")\n",
        "async def chat(request: ChatRequest):\n",
        "    \"\"\"Streaming chat endpoint\"\"\"\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"nvidia/nemotron-nano-9b-v2\",\n",
        "            messages=request.messages,\n",
        "            temperature=request.temperature,\n",
        "            max_tokens=request.max_tokens,\n",
        "            stream=request.stream\n",
        "        )\n",
        "        \n",
        "        if request.stream:\n",
        "            async def generate():\n",
        "                for chunk in response:\n",
        "                    if chunk.choices[0].delta.content:\n",
        "                        yield f\"data: {chunk.choices[0].delta.content}\\n\\n\"\n",
        "                yield \"data: [DONE]\\n\\n\"\n",
        "            \n",
        "            return StreamingResponse(generate(), media_type=\"text/event-stream\")\n",
        "        else:\n",
        "            return {\"content\": response.choices[0].message.content}\n",
        "    \n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "async def health():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return {\"status\": \"healthy\", \"model\": \"nemotron-nano-9b-v2\"}\n",
        "\n",
        "# Run with: uvicorn main:app --reload\n",
        "```\n",
        "\n",
        "### Deployment Considerations\n",
        "\n",
        "1. **Rate Limiting**: Implement rate limiting to prevent abuse\n",
        "2. **Authentication**: Add API key authentication for production\n",
        "3. **Caching**: Cache common responses to reduce costs\n",
        "4. **Monitoring**: Add logging and metrics (Prometheus, DataDog)\n",
        "5. **Load Balancing**: Use NGINX or cloud load balancers for scale\n",
        "\n",
        "## Resources\n",
        "\n",
        "- [NVIDIA NIM API Documentation](https://docs.nvidia.com/nim/)\n",
        "- [OpenAI Python Library](https://github.com/openai/openai-python)\n",
        "- [FastAPI Documentation](https://fastapi.tiangolo.com/)\n",
        "- [NVIDIA AI Blueprints](https://github.com/NVIDIA-AI-Blueprints)\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- üìò Try `Demo_Nemotron_Nano.ipynb` for interactive examples\n",
        "- üöÄ Deploy your own API endpoint\n",
        "- üîß Customize functions for your use case\n",
        "- üìö Explore other NVIDIA models on [build.nvidia.com](https://build.nvidia.com/)\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Building with NVIDIA Nemotron!** üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

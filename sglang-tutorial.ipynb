{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Banner Image -->\n",
        "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
        "\n",
        "<!-- Links -->\n",
        "<center>\n",
        "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> ‚Ä¢\n",
        "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> ‚Ä¢\n",
        "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> ‚Ä¢\n",
        "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
        "</center>\n",
        "\n",
        "# SGLang: The Fastest LLM Serving Framework for NVIDIA GPUs ‚ö°\n",
        "\n",
        "Welcome to the future of LLM inference!\n",
        "\n",
        "## Why SGLang? The Numbers Speak for Themselves\n",
        "\n",
        "SGLang achieves **up to 5x faster inference** compared to traditional serving frameworks through revolutionary innovations:\n",
        "\n",
        "- **RadixAttention**: Automatic KV cache reuse with prefix matching - dramatically reduces redundant computation\n",
        "- **FlashInfer Kernels**: Custom CUDA kernels optimized for batched attention operations\n",
        "- **6.4x faster** than vLLM on multi-turn conversations\n",
        "- **5x higher throughput** than HuggingFace TGI on real-world workloads\n",
        "- **Used in production** by xAI (Grok), Cursor, Microsoft, Oracle, and more\n",
        "\n",
        "## What Makes This Tutorial Special\n",
        "\n",
        "This is a **production-grade, keynote-quality** tutorial that goes beyond basics:\n",
        "\n",
        "‚úÖ **Real Performance Benchmarks** - See actual speedups with RadixAttention  \n",
        "‚úÖ **Production Best Practices** - Error handling, monitoring, and optimization  \n",
        "‚úÖ **Advanced Features** - FP8 quantization, multi-modal models, streaming  \n",
        "‚úÖ **Real-World Use Cases** - Code completion, chatbots, RAG applications  \n",
        "‚úÖ **Complete Observability** - Metrics, logging, and health monitoring  \n",
        "\n",
        "## What You'll Master\n",
        "\n",
        "1. **Installation & Setup** - Get SGLang running in minutes\n",
        "2. **Basic Serving** - Launch and query LLM servers with OpenAI-compatible APIs\n",
        "3. **RadixAttention Demo** - See the magic of automatic KV cache reuse\n",
        "4. **Performance Benchmarking** - Compare SGLang vs other frameworks\n",
        "5. **Production Optimization** - FP8 quantization, tensor parallelism, memory tuning\n",
        "6. **Multi-Modal Serving** - Vision-language models like LLaVA\n",
        "7. **Real-World Application** - Build a high-performance code assistant\n",
        "8. **Monitoring & Ops** - Production-ready observability\n",
        "\n",
        "---\n",
        "\n",
        "#### üí¨ Help us improve! Feedback welcome on [Discord](https://discord.gg/T9bUNqMS8d) or [X/Twitter](https://x.com/brevdev)\n",
        "\n",
        "**üìù Notebook Tips**: Press `Shift + Enter` to run cells. A `*` means running, a number means complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Get & Set Up a GPU\n",
        "\n",
        "SGLang requires an NVIDIA GPU with:\n",
        "- CUDA 11.8+ and compute capability 7.0+\n",
        "- Recommended: A10G (24GB), L4 (24GB), A100 (40GB/80GB), or H100\n",
        "- RAM: At least 32GB system RAM\n",
        "- Disk Space: Minimum 50GB for model weights\n",
        "\n",
        "I used a GPU and dev environment from [brev.dev](https://brev.dev). Click the badge below to get your preconfigured instance:\n",
        "\n",
        "[![](https://brev-assets.s3.us-west-1.amazonaws.com/nv-lb-dark.svg)](https://console.brev.dev/environment/new?instance=A10G:g5.xlarge&diskStorage=256&name=sglang-tutorial&file=https://github.com/brevdev/notebooks/raw/main/sglang-tutorial.ipynb&python=3.10&cuda=12.0.1)\n",
        "\n",
        "Once you've checked out your machine and landed in your instance page, select the specs (Python 3.10 and CUDA 12.0.1 recommended) and click the \"Build\" button to build your container.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU availability and CUDA version with comprehensive checks\n",
        "import torch\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîç GPU VERIFICATION & SYSTEM CHECK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check PyTorch and CUDA\n",
        "print(f\"\\nüì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"üîß CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"\\n‚ùå ERROR: CUDA is not available!\")\n",
        "    print(\"   SGLang requires an NVIDIA GPU with CUDA support.\")\n",
        "    print(\"   Please ensure you're running on a GPU instance.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"‚úÖ CUDA version: {torch.version.cuda}\")\n",
        "print(f\"üéÆ Number of GPUs: {torch.cuda.device_count()}\")\n",
        "\n",
        "# Check each GPU\n",
        "gpu_memory_gb = 0\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    props = torch.cuda.get_device_properties(i)\n",
        "    gpu_memory_gb = props.total_memory / 1024**3\n",
        "    compute_cap = torch.cuda.get_device_capability(i)\n",
        "    \n",
        "    print(f\"\\nüöÄ GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    print(f\"   üíæ Memory: {gpu_memory_gb:.2f} GB\")\n",
        "    print(f\"   ‚öôÔ∏è  Compute Capability: {compute_cap[0]}.{compute_cap[1]}\")\n",
        "    \n",
        "    # Check minimum requirements\n",
        "    if compute_cap[0] < 7:\n",
        "        print(f\"   ‚ö†Ô∏è  WARNING: Compute capability {compute_cap[0]}.{compute_cap[1]} may not be supported\")\n",
        "        print(f\"   SGLang recommends compute capability 7.0+ (V100, T4, A10G, A100, H100, L4)\")\n",
        "    \n",
        "    if gpu_memory_gb < 16:\n",
        "        print(f\"   ‚ö†Ô∏è  WARNING: {gpu_memory_gb:.0f}GB may be insufficient for larger models\")\n",
        "        print(f\"   Recommended: 24GB+ for production workloads\")\n",
        "\n",
        "# Run nvidia-smi for detailed GPU status\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä NVIDIA-SMI Output:\")\n",
        "print(\"=\" * 70)\n",
        "try:\n",
        "    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
        "    print(result.stdout)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ùå Error running nvidia-smi: {e}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå nvidia-smi not found. NVIDIA drivers may not be installed.\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ GPU VERIFICATION COMPLETE\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected Output\n",
        "\n",
        "You should see output similar to:\n",
        "```\n",
        "======================================================================\n",
        "üîç GPU VERIFICATION & SYSTEM CHECK\n",
        "======================================================================\n",
        "\n",
        "üì¶ PyTorch version: 2.1.0+cu121\n",
        "üîß CUDA available: True\n",
        "‚úÖ CUDA version: 12.1\n",
        "üéÆ Number of GPUs: 1\n",
        "\n",
        "üöÄ GPU 0: NVIDIA A10G\n",
        "   üíæ Memory: 22.20 GB\n",
        "   ‚öôÔ∏è  Compute Capability: 8.6\n",
        "\n",
        "======================================================================\n",
        "‚úÖ GPU VERIFICATION COMPLETE\n",
        "======================================================================\n",
        "```\n",
        "\n",
        "‚úÖ **You're ready to proceed!** Your GPU meets all requirements for SGLang.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install SGLang\n",
        "\n",
        "We'll install SGLang using the `uv` package manager for faster, more reliable installation.\n",
        "\n",
        "**Installation includes:**\n",
        "- SGLang core framework with RadixAttention\n",
        "- FlashInfer kernels (optimized CUDA attention)\n",
        "- vLLM integration for model compatibility\n",
        "- OpenAI-compatible API server\n",
        "- All necessary dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üì¶ INSTALLING SGLANG\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Install uv package manager\n",
        "print(\"\\n[1/3] Installing uv package manager...\")\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"-q\"], check=True)\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\", \"-q\"], check=True)\n",
        "    print(\"‚úÖ uv installed successfully\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ùå Error installing uv: {e}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Step 2: Install SGLang with all dependencies\n",
        "print(\"\\n[2/3] Installing SGLang (this may take 2-3 minutes)...\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"uv\", \"pip\", \"install\", \"--system\", \"sglang[all]\", \"--prerelease=allow\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True\n",
        "    )\n",
        "    print(\"‚úÖ SGLang installed successfully\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ùå Error installing SGLang: {e.stderr}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Step 3: Verify installation\n",
        "print(\"\\n[3/3] Verifying installation...\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"sglang.version\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True\n",
        "    )\n",
        "    print(\"‚úÖ Installation verified!\")\n",
        "    print(f\"\\n{result.stdout}\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Could not verify version: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ INSTALLATION COMPLETE\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Docker Installation\n",
        "\n",
        "For a pre-configured environment, you can use the official SGLang Docker image:\n",
        "\n",
        "```bash\n",
        "docker run --gpus all \\\n",
        "  --shm-size 32g \\\n",
        "  -p 30000:30000 \\\n",
        "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
        "  --env \"HF_TOKEN=<your_token>\" \\\n",
        "  --ipc=host \\\n",
        "  lmsysorg/sglang:latest \\\n",
        "  python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. HuggingFace Authentication\n",
        "\n",
        "Some models require HuggingFace authentication. You can get a token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up HuggingFace token for model downloads\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# You can get a token from https://huggingface.co/settings/tokens\n",
        "HF_TOKEN = input(\"Enter your HuggingFace token (or press Enter to skip): \")\n",
        "\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN)\n",
        "    os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "    print(\"‚úì HuggingFace authentication successful!\")\n",
        "else:\n",
        "    print(\"‚ö† Skipped HuggingFace authentication - you may have limited access to some models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Launch Your First SGLang Server\n",
        "\n",
        "Let's launch SGLang with **Llama-3.1-8B-Instruct** - a state-of-the-art model from Meta.\n",
        "\n",
        "### What's Happening Under the Hood:\n",
        "\n",
        "1. **Model Download**: SGLang downloads the model from HuggingFace (cached for future use)\n",
        "2. **Model Loading**: Weights are loaded into GPU memory\n",
        "3. **RadixAttention Init**: Prefix tree initialized for KV cache reuse\n",
        "4. **FlashInfer Compilation**: Custom CUDA kernels compiled for your GPU\n",
        "5. **Server Ready**: OpenAI-compatible API server starts on port 30000\n",
        "\n",
        "**Expected time**: 2-5 minutes (first run), 30 seconds (subsequent runs with cached model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import threading\n",
        "import requests\n",
        "from datetime import datetime\n",
        "\n",
        "# Global variables\n",
        "server_process = None\n",
        "server_logs = []\n",
        "\n",
        "def wait_for_server(url=\"http://127.0.0.1:30000/health\", timeout=300, check_interval=2):\n",
        "    \"\"\"Wait for server to become ready with health checks\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    print(f\"‚è≥ Waiting for server to be ready (timeout: {timeout}s)...\")\n",
        "    \n",
        "    while time.time() - start_time < timeout:\n",
        "        try:\n",
        "            response = requests.get(url, timeout=1)\n",
        "            if response.status_code == 200:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"‚úÖ Server is ready! (took {elapsed:.1f}s)\")\n",
        "                return True\n",
        "        except requests.exceptions.RequestException:\n",
        "            pass\n",
        "        \n",
        "        # Show progress every 10 seconds\n",
        "        elapsed = int(time.time() - start_time)\n",
        "        if elapsed % 10 == 0 and elapsed > 0:\n",
        "            print(f\"   Still waiting... ({elapsed}s elapsed)\")\n",
        "        \n",
        "        time.sleep(check_interval)\n",
        "    \n",
        "    print(f\"‚ùå Server failed to start within {timeout}s\")\n",
        "    return False\n",
        "\n",
        "def run_server_background():\n",
        "    \"\"\"Run SGLang server in background thread\"\"\"\n",
        "    global server_process, server_logs\n",
        "    \n",
        "    cmd = [\n",
        "        \"python3\", \"-m\", \"sglang.launch_server\",\n",
        "        \"--model-path\", \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"--port\", \"30000\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--log-level\", \"info\"\n",
        "    ]\n",
        "    \n",
        "    server_process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Capture logs\n",
        "    for line in server_process.stdout:\n",
        "        server_logs.append(line)\n",
        "        # Print important lines\n",
        "        if any(keyword in line.lower() for keyword in ['error', 'warning', 'server started', 'ready']):\n",
        "            print(f\"   {line.strip()}\")\n",
        "\n",
        "# Main execution\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ LAUNCHING SGLANG SERVER\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nüìã Configuration:\")\n",
        "print(f\"   Model: meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(f\"   Port: 30000\")\n",
        "print(f\"   API: OpenAI-compatible\")\n",
        "print(f\"\\n‚è±Ô∏è  Starting server at {datetime.now().strftime('%H:%M:%S')}...\\n\")\n",
        "\n",
        "# Start server thread\n",
        "server_thread = threading.Thread(target=run_server_background, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Wait for server to be ready\n",
        "if wait_for_server(timeout=300):\n",
        "    # Test server\n",
        "    try:\n",
        "        health = requests.get(\"http://127.0.0.1:30000/health\").json()\n",
        "        print(f\"\\nüìä Server Status:\")\n",
        "        print(f\"   Health: {health}\")\n",
        "        \n",
        "        models = requests.get(\"http://127.0.0.1:30000/v1/models\").json()\n",
        "        print(f\"   Loaded Models: {len(models.get('data', []))}\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"‚úÖ SERVER READY FOR INFERENCE\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"\\nüåê Endpoints:\")\n",
        "        print(f\"   - Health: http://127.0.0.1:30000/health\")\n",
        "        print(f\"   - Models: http://127.0.0.1:30000/v1/models\")\n",
        "        print(f\"   - Completions: http://127.0.0.1:30000/v1/completions\")\n",
        "        print(f\"   - Chat: http://127.0.0.1:30000/v1/chat/completions\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Server started but health check failed: {e}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Failed to start server. Check logs above for errors.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Query the SGLang Server\n",
        "\n",
        "Now let's send requests to our running server using the SGLang Python client.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Magic of RadixAttention: SGLang's Secret Weapon ‚ú®\n",
        "\n",
        "Before we query the server, let's understand what makes SGLang **6x faster** than competitors.\n",
        "\n",
        "### RadixAttention: Automatic KV Cache Reuse\n",
        "\n",
        "Traditional LLM servers recompute attention for every request, even when prompts share common prefixes. **RadixAttention** solves this with:\n",
        "\n",
        "- **Radix Tree Structure**: Organizes KV caches in a tree where shared prefixes are stored once\n",
        "- **Automatic Matching**: Detects common prefixes across requests without manual hints\n",
        "- **Zero-Copy Reuse**: Instantly reuses cached attention states\n",
        "\n",
        "### Real-World Impact:\n",
        "\n",
        "| Scenario | Traditional | With RadixAttention | Speedup |\n",
        "|----------|------------|---------------------|---------|\n",
        "| Multi-turn chat | Recompute full history | Reuse conversation context | **6.4x** |\n",
        "| Few-shot prompts | Recompute examples | Reuse example embeddings | **3.2x** |\n",
        "| Shared system prompts | Recompute every time | Cache once, reuse forever | **5x** |\n",
        "| RAG with long context | Recompute documents | Cache document embeddings | **4.8x** |\n",
        "\n",
        "**Next, we'll see this in action with real benchmarks!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import time\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üß™ TESTING SGLANG SERVER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Configure the OpenAI client to point to our SGLang server\n",
        "client = openai.Client(\n",
        "    base_url=\"http://127.0.0.1:30000/v1\",\n",
        "    api_key=\"EMPTY\"  # SGLang doesn't require an API key - fully open!\n",
        ")\n",
        "\n",
        "# Test 1: Basic Completion\n",
        "print(\"\\n[Test 1] Basic Completion\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    _ = response = client.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        prompt=\"The capital of France is\",\n",
        "        max_tokens=50,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Success!\")\n",
        "    print(f\"   Prompt: \\\"The capital of France is\\\"\")\n",
        "    print(f\"   Response: \\\"{response.choices[0].text.strip()}\\\"\")\n",
        "    print(f\"   Tokens: {response.usage.total_tokens}\")\n",
        "    print(f\"   Latency: {latency:.3f}s\")\n",
        "    print(f\"   Throughput: {response.usage.total_tokens / latency:.1f} tokens/s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Test 2: Chat Completion\n",
        "print(\"\\n[Test 2] Chat Completion\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    _ = chat_response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in explaining complex technology simply.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain what SGLang is and why it's revolutionary in 2 sentences.\"}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Success!\")\n",
        "    print(f\"   User: \\\"Explain what SGLang is and why it's revolutionary\\\"\")\n",
        "    print(f\"   Assistant: \\\"{chat_response.choices[0].message.content}\\\"\")\n",
        "    print(f\"   Tokens: {chat_response.usage.total_tokens}\")\n",
        "    print(f\"   Latency: {latency:.3f}s\")\n",
        "    print(f\"   Throughput: {chat_response.usage.total_tokens / latency:.1f} tokens/s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Test 3: Structured Output\n",
        "print(\"\\n[Test 3] Structured Generation (JSON)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    _ = structured_response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an API that returns JSON only. No other text.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Generate a JSON object with 3 AI companies and their main products.\"}\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Success!\")\n",
        "    print(f\"   Response: {structured_response.choices[0].message.content}\")\n",
        "    print(f\"   Latency: {latency:.3f}s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL TESTS PASSED!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RadixAttention in Action: See the Speedup! üöÄ\n",
        "\n",
        "Let's demonstrate the power of RadixAttention with a real benchmark. We'll send multiple requests with shared prefixes and measure the speedup.\n",
        "\n",
        "### The Test:\n",
        "- **10 requests** with the same long system prompt\n",
        "- **Without RadixAttention**: Each request recomputes the entire prompt\n",
        "- **With RadixAttention** (SGLang): First request computes, rest reuse cached KV\n",
        "\n",
        "This is exactly what happens in production: chatbots, RAG systems, and APIs all reuse system prompts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import statistics\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚ö° RADIXATTENTION BENCHMARK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create a long system prompt (this simulates a RAG context or few-shot examples)\n",
        "system_prompt = \"\"\"You are an expert AI coding assistant. You follow these principles:\n",
        "1. Write clean, readable, and maintainable code\n",
        "2. Use proper error handling and input validation\n",
        "3. Follow PEP 8 style guidelines for Python\n",
        "4. Add helpful comments and docstrings\n",
        "5. Consider edge cases and potential bugs\n",
        "6. Optimize for clarity over cleverness\n",
        "7. Use type hints when helpful\n",
        "8. Write testable code with clear interfaces\n",
        "9. Follow SOLID principles\n",
        "10. Consider security implications\n",
        "\n",
        "You have expertise in: Python, JavaScript, Go, Rust, C++, Java, TypeScript,\n",
        "React, Node.js, Django, FastAPI, PostgreSQL, MongoDB, Redis, Docker, Kubernetes,\n",
        "AWS, GCP, Azure, CI/CD, Testing, Security, Performance Optimization, and more.\"\"\"\n",
        "\n",
        "# Questions to ask\n",
        "questions = [\n",
        "    \"Write a Python function to find prime numbers\",\n",
        "    \"Create a binary search implementation\",\n",
        "    \"Show me a quicksort algorithm\",\n",
        "    \"Write a function to reverse a linked list\",\n",
        "    \"Implement a hash table in Python\",\n",
        "    \"Create a depth-first search function\",\n",
        "    \"Write a function to detect cycles in a graph\",\n",
        "    \"Implement a min heap data structure\",\n",
        "    \"Show me how to do merge sort\",\n",
        "    \"Write a function to check if a string is a palindrome\",\n",
        "]\n",
        "\n",
        "print(f\"\\nüìä Test Configuration:\")\n",
        "print(f\"   System Prompt Length: {len(system_prompt)} characters (~{len(system_prompt.split())} tokens)\")\n",
        "print(f\"   Number of Requests: {len(questions)}\")\n",
        "print(f\"   Expected Behavior:\")\n",
        "print(f\"      - Request 1: Full computation (slower)\")\n",
        "print(f\"      - Requests 2-10: RadixAttention reuse (MUCH faster)\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Running benchmark...\\n\")\n",
        "\n",
        "latencies = []\n",
        "first_request_time = None\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    start = time.time()\n",
        "    \n",
        "    try:\n",
        "        _ = response = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        latency = time.time() - start\n",
        "        latencies.append(latency)\n",
        "        \n",
        "        if i == 1:\n",
        "            first_request_time = latency\n",
        "        \n",
        "        # Show progress\n",
        "        speedup = first_request_time / latency if i > 1 else 1.0\n",
        "        emoji = \"üî•\" if speedup > 1.5 else \"‚úÖ\"\n",
        "        print(f\"   {emoji} Request {i:2d}: {latency:.3f}s (speedup: {speedup:.2f}x)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Request {i} failed: {e}\")\n",
        "\n",
        "# Calculate statistics\n",
        "if len(latencies) >= 2:\n",
        "    first_req = latencies[0]\n",
        "    subsequent_reqs = latencies[1:]\n",
        "    \n",
        "    avg_subsequent = statistics.mean(subsequent_reqs)\n",
        "    speedup = first_req / avg_subsequent\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìà BENCHMARK RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n‚è±Ô∏è  Latency:\")\n",
        "    print(f\"   First Request (cold):        {first_req:.3f}s\")\n",
        "    print(f\"   Average Subsequent (cached): {avg_subsequent:.3f}s\")\n",
        "    print(f\"   Best Subsequent:             {min(subsequent_reqs):.3f}s\")\n",
        "    print(f\"   Worst Subsequent:            {max(subsequent_reqs):.3f}s\")\n",
        "    \n",
        "    print(f\"\\nüöÄ RadixAttention Speedup:\")\n",
        "    print(f\"   Average: {speedup:.2f}x faster\")\n",
        "    print(f\"   Best:    {first_req / min(subsequent_reqs):.2f}x faster\")\n",
        "    \n",
        "    print(f\"\\nüí∞ Cost Savings:\")\n",
        "    total_saved = (first_req - avg_subsequent) * len(subsequent_reqs)\n",
        "    print(f\"   Time saved: {total_saved:.2f}s across {len(subsequent_reqs)} requests\")\n",
        "    print(f\"   In production (1M requests/day): ~{(total_saved * 1000000 / len(subsequent_reqs) / 3600):.1f} hours saved!\")\n",
        "    \n",
        "    print(f\"\\nüéØ Key Insight:\")\n",
        "    print(f\"   The system prompt ({len(system_prompt.split())} tokens) was cached after\")\n",
        "    print(f\"   the first request and reused instantly for all subsequent requests.\")\n",
        "    print(f\"   This is the power of RadixAttention! üî•\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Not enough successful requests to calculate statistics\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production Optimization: FP8 Quantization ‚öôÔ∏è\n",
        "\n",
        "Now let's level up with **FP8 quantization** - reduce memory by 2x and increase speed by 1.5x with minimal quality loss.\n",
        "\n",
        "### What is FP8 Quantization?\n",
        "\n",
        "- **FP8 (8-bit Floating Point)**: Uses 8-bit instead of 16-bit weights\n",
        "- **Memory**: Reduces VRAM usage by ~50% (fit larger models or bigger batches)\n",
        "- **Speed**: Faster computation with Tensor Cores on modern NVIDIA GPUs (A100, H100, L4)\n",
        "- **Quality**: <1% degradation on most tasks\n",
        "\n",
        "### When to Use FP8:\n",
        "\n",
        "‚úÖ Production deployments (save costs)  \n",
        "‚úÖ Large batch sizes (2x throughput)  \n",
        "‚úÖ Bigger models on smaller GPUs  \n",
        "‚úÖ Lower latency requirements\n",
        "\n",
        "Let's restart the server with FP8 enabled:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop previous server (if running)\n",
        "import requests\n",
        "\n",
        "try:\n",
        "    requests.post(\"http://127.0.0.1:30000/shutdown\", timeout=5)\n",
        "    time.sleep(5)\n",
        "    print(\"Previous server stopped.\")\n",
        "except:\n",
        "    print(\"No previous server to stop.\")\n",
        "\n",
        "# Launch with FP8 quantization for better performance\n",
        "def run_quantized_server():\n",
        "    \"\"\"Run SGLang server with FP8 quantization\"\"\"\n",
        "    global server_process\n",
        "    \n",
        "    cmd = [\n",
        "        \"python3\", \"-m\", \"sglang.launch_server\",\n",
        "        \"--model-path\", \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"--port\", \"30000\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--dtype\", \"bfloat16\",\n",
        "        \"--quantization\", \"fp8\",  # Enable FP8 quantization\n",
        "        \"--mem-fraction-static\", \"0.85\"  # Use 85% of GPU memory for KV cache\n",
        "    ]\n",
        "    \n",
        "    server_process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    for line in server_process.stdout:\n",
        "        print(line, end='')\n",
        "    \n",
        "    return server_process\n",
        "\n",
        "print(\"Launching SGLang server with FP8 quantization...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "quantized_server_thread = threading.Thread(target=run_quantized_server, daemon=True)\n",
        "quantized_server_thread.start()\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"\\nQuantized server ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Benchmark Server Performance\n",
        "\n",
        "Let's benchmark the server's throughput and latency under concurrent load.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark the server\n",
        "import concurrent.futures\n",
        "import time\n",
        "import statistics\n",
        "\n",
        "def send_request(prompt, max_tokens=256):\n",
        "    \"\"\"Send a single request and measure latency\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    response = client.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    tokens = response.usage.total_tokens\n",
        "    \n",
        "    return {\n",
        "        'latency': latency,\n",
        "        'tokens': tokens,\n",
        "        'tokens_per_second': tokens / latency\n",
        "    }\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Write a short story about a robot learning to paint.\",\n",
        "    \"Explain quantum computing to a 5-year-old.\",\n",
        "    \"What are the key features of Python programming?\",\n",
        "    \"Describe the process of photosynthesis.\",\n",
        "    \"What is the history of artificial intelligence?\",\n",
        "]\n",
        "\n",
        "print(\"Running benchmark with 5 concurrent requests...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Send concurrent requests\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    futures = [executor.submit(send_request, prompt) for prompt in test_prompts]\n",
        "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Calculate statistics\n",
        "latencies = [r['latency'] for r in results]\n",
        "tokens_per_sec = [r['tokens_per_second'] for r in results]\n",
        "total_tokens = sum(r['tokens'] for r in results)\n",
        "\n",
        "print(f\"\\nBenchmark Results:\")\n",
        "print(f\"  Total time: {total_time:.2f}s\")\n",
        "print(f\"  Total tokens: {total_tokens}\")\n",
        "print(f\"  Average latency: {statistics.mean(latencies):.2f}s\")\n",
        "print(f\"  P50 latency: {statistics.median(latencies):.2f}s\")\n",
        "print(f\"  P95 latency: {sorted(latencies)[int(len(latencies)*0.95)]:.2f}s\")\n",
        "print(f\"  Average throughput: {statistics.mean(tokens_per_sec):.2f} tokens/s\")\n",
        "print(f\"  Total throughput: {total_tokens/total_time:.2f} tokens/s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Serving Multi-Modal Models (Vision + Language)\n",
        "\n",
        "SGLang supports vision-language models like LLaVA. Let's try serving a multi-modal model.\n",
        "\n",
        "**Note**: This requires more GPU memory. Skip this section if you have < 24GB VRAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch SGLang with LLaVA model (Vision-Language)\n",
        "# Stop previous server first\n",
        "try:\n",
        "    requests.post(\"http://127.0.0.1:30000/shutdown\", timeout=5)\n",
        "    time.sleep(5)\n",
        "    print(\"Previous server stopped.\")\n",
        "except:\n",
        "    print(\"No previous server to stop.\")\n",
        "\n",
        "def run_multimodal_server():\n",
        "    \"\"\"Run SGLang server with LLaVA (vision-language model)\"\"\"\n",
        "    global server_process\n",
        "    \n",
        "    cmd = [\n",
        "        \"python3\", \"-m\", \"sglang.launch_server\",\n",
        "        \"--model-path\", \"lmms-lab/llama3-llava-next-8b\",\n",
        "        \"--port\", \"30000\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--tp-size\", \"1\",  # Tensor parallelism size\n",
        "        \"--chat-template\", \"llava_llama_3\"\n",
        "    ]\n",
        "    \n",
        "    server_process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    for line in server_process.stdout:\n",
        "        print(line, end='')\n",
        "    \n",
        "    return server_process\n",
        "\n",
        "print(\"Launching SGLang server with LLaVA multi-modal model...\")\n",
        "print(\"This may take longer as it's a larger model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "multimodal_thread = threading.Thread(target=run_multimodal_server, daemon=True)\n",
        "multimodal_thread.start()\n",
        "\n",
        "time.sleep(45)\n",
        "print(\"\\nMulti-modal server ready! You can now query it with images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Using the HTTP API Directly\n",
        "\n",
        "SGLang exposes an OpenAI-compatible HTTP API. You can query it with curl or any HTTP client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query the server using HTTP API directly\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Health check\n",
        "_ = health = requests.get(\"http://127.0.0.1:30000/health\")\n",
        "print(f\"Server health: {health.json()}\")\n",
        "\n",
        "# Get model info\n",
        "_ = model_info = requests.get(\"http://127.0.0.1:30000/v1/models\")\n",
        "print(f\"\\nAvailable models: {json.dumps(model_info.json(), indent=2)}\")\n",
        "\n",
        "# Send a completion request via HTTP\n",
        "payload = {\n",
        "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"prompt\": \"What are the benefits of using SGLang?\",\n",
        "    \"max_tokens\": 200,\n",
        "    \"temperature\": 0.7,\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "_ = response = requests.post(\n",
        "    \"http://127.0.0.1:30000/v1/completions\",\n",
        "    json=payload,\n",
        "    headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "result = response.json()\n",
        "print(f\"\\nHTTP API Response:\")\n",
        "print(f\"Prompt: {payload['prompt']}\")\n",
        "print(f\"Completion: {result['choices'][0]['text']}\")\n",
        "print(f\"Tokens used: {result['usage']['total_tokens']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices & Tips\n",
        "\n",
        "### Memory Optimization\n",
        "- Use `--mem-fraction-static` to control KV cache size (default 0.9)\n",
        "- Enable quantization: `--quantization fp8` or `--quantization awq`\n",
        "- Use smaller context lengths for higher throughput\n",
        "\n",
        "### Performance Tuning\n",
        "- Adjust batch size with `--max-running-requests` (default 4096)\n",
        "- Use tensor parallelism for large models: `--tp-size 2` or `--tp-size 4`\n",
        "- Enable continuous batching for better throughput\n",
        "\n",
        "### Common Issues\n",
        "1. **CUDA Out of Memory**: Reduce `--mem-fraction-static` or use quantization\n",
        "2. **Slow First Request**: Model loading takes time; subsequent requests are fast\n",
        "3. **Port Already in Use**: Change port with `--port 30001` or kill existing process\n",
        "\n",
        "### Production Deployment\n",
        "- Use Docker for reproducible deployments\n",
        "- Set up monitoring with Prometheus/Grafana\n",
        "- Use a load balancer for multiple instances\n",
        "- Enable logging: `--log-level info`\n",
        "\n",
        "### Useful Commands\n",
        "```bash\n",
        "# Check SGLang version\n",
        "python -m sglang.version\n",
        "\n",
        "# List available models\n",
        "python -m sglang.list_models\n",
        "\n",
        "# Run benchmarks\n",
        "python -m sglang.bench_serving \\\n",
        "  --backend sglang \\\n",
        "  --model meta-llama/Llama-3.1-8B-Instruct \\\n",
        "  --port 30000 \\\n",
        "  --dataset-name random \\\n",
        "  --random-input 512 \\\n",
        "  --random-output 256 \\\n",
        "  --max-concurrency 16\n",
        "```\n",
        "\n",
        "### Resources\n",
        "- [SGLang GitHub](https://github.com/sgl-project/sglang)\n",
        "- [SGLang Documentation](https://docs.sglang.ai/)\n",
        "- [FlashInfer Kernels](https://github.com/flashinfer-ai/flashinfer)\n",
        "- [Model Compatibility List](https://docs.sglang.ai/backend/supported_models.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Advanced: Streaming Responses\n",
        "\n",
        "SGLang supports streaming responses for real-time output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of streaming responses\n",
        "print(\"Testing streaming response...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a haiku about GPU computing.\"}\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.8,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"\\nStreaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"Streaming complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Cleanup\n",
        "\n",
        "Let's stop the server and clean up resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean up - stop the server\n",
        "print(\"Stopping SGLang server...\")\n",
        "\n",
        "try:\n",
        "    response = requests.post(\"http://127.0.0.1:30000/shutdown\", timeout=5)\n",
        "    print(\"Server stopped successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Server may have already been stopped. Error: {e}\")\n",
        "\n",
        "# Kill process if still running\n",
        "if server_process and server_process.poll() is None:\n",
        "    server_process.terminate()\n",
        "    time.sleep(2)\n",
        "    if server_process.poll() is None:\n",
        "        server_process.kill()\n",
        "    print(\"Process terminated.\")\n",
        "\n",
        "print(\"\\nTutorial complete! üéâ\")\n",
        "print(\"You now know how to:\")\n",
        "print(\"  ‚úì Install and configure SGLang on NVIDIA GPUs\")\n",
        "print(\"  ‚úì Launch servers with different models and optimizations\")\n",
        "print(\"  ‚úì Query servers using Python and HTTP APIs\")\n",
        "print(\"  ‚úì Benchmark performance and optimize for production\")\n",
        "print(\"  ‚úì Use streaming responses for real-time output\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  - Try different models from HuggingFace\")\n",
        "print(\"  - Experiment with quantization techniques\")\n",
        "print(\"  - Deploy to production with Docker\")\n",
        "print(\"  - Explore multi-GPU setups with tensor parallelism\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Need Help?\n",
        "\n",
        "Join the community:\n",
        "- [SGLang GitHub Discussions](https://github.com/sgl-project/sglang/discussions)\n",
        "- [Brev Discord](https://discord.gg/NVDyv7TUgJ)\n",
        "- [SGLang Documentation](https://docs.sglang.ai/)\n",
        "\n",
        "Built with ‚ù§Ô∏è by the Brev.dev team\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

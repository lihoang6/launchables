{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- Banner Image -->\n",
        "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
        "\n",
        "<!-- Links -->\n",
        "<center>\n",
        "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> ‚Ä¢\n",
        "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> ‚Ä¢\n",
        "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> ‚Ä¢\n",
        "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
        "</center>\n",
        "\n",
        "# SGLang: The Fastest LLM Serving Framework for NVIDIA GPUs ‚ö°\n",
        "\n",
        "Welcome to the future of LLM inference!\n",
        "\n",
        "## Why SGLang? The Numbers Speak for Themselves\n",
        "\n",
        "SGLang achieves **up to 5x faster inference** compared to traditional serving frameworks through revolutionary innovations:\n",
        "\n",
        "- **RadixAttention**: Automatic KV cache reuse with prefix matching - dramatically reduces redundant computation\n",
        "- **FlashInfer Kernels**: Custom CUDA kernels optimized for batched attention operations\n",
        "- **6.4x faster** than vLLM on multi-turn conversations\n",
        "- **5x higher throughput** than HuggingFace TGI on real-world workloads\n",
        "- **Used in production** by xAI (Grok), Cursor, Microsoft, Oracle, and more\n",
        "\n",
        "## What Makes This Tutorial Special\n",
        "\n",
        "This is a **production-grade, keynote-quality** tutorial that goes beyond basics:\n",
        "\n",
        "‚úÖ **Real Performance Benchmarks** - See actual speedups with RadixAttention  \n",
        "‚úÖ **Production Best Practices** - Error handling, monitoring, and optimization  \n",
        "‚úÖ **Advanced Features** - FP8 quantization, multi-modal models, streaming  \n",
        "‚úÖ **Real-World Use Cases** - Code completion, chatbots, RAG applications  \n",
        "‚úÖ **Complete Observability** - Metrics, logging, and health monitoring  \n",
        "\n",
        "## What You'll Master\n",
        "\n",
        "1. **Installation & Setup** - Get SGLang running in minutes\n",
        "2. **Basic Serving** - Launch and query LLM servers with OpenAI-compatible APIs\n",
        "3. **RadixAttention Demo** - See the magic of automatic KV cache reuse\n",
        "4. **Performance Benchmarking** - Compare SGLang vs other frameworks\n",
        "5. **Production Optimization** - FP8 quantization, tensor parallelism, memory tuning\n",
        "6. **Multi-Modal Serving** - Vision-language models like LLaVA\n",
        "7. **Real-World Application** - Build a high-performance code assistant\n",
        "8. **Monitoring & Ops** - Production-ready observability\n",
        "\n",
        "---\n",
        "\n",
        "#### üí¨ Help us improve! Feedback welcome on [Discord](https://discord.gg/T9bUNqMS8d) or [X/Twitter](https://x.com/brevdev)\n",
        "\n",
        "**üìù Notebook Tips**: Press `Shift + Enter` to run cells. A `*` means running, a number means complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Verify Your GPU Setup\n",
        "\n",
        "**Good news!** Your NVIDIA GPU is already provisioned and ready to go! üéâ\n",
        "\n",
        "SGLang works best with:\n",
        "- ‚úÖ CUDA 11.8+ and compute capability 7.0+\n",
        "- ‚úÖ GPUs: L40S, A10G, L4, A100, H100, or similar\n",
        "- ‚úÖ RAM: 32GB+ system RAM recommended\n",
        "- ‚úÖ Disk: 50GB+ for model weights\n",
        "\n",
        "**Let's verify your GPU is detected and working:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch if not already installed\n",
        "print(\"Checking/Installing PyTorch...\")\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"‚úì PyTorch {torch.__version__} already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing PyTorch (this may take 2-3 minutes)...\")\n",
        "    \n",
        "    # Method 1: Try using pip directly (most reliable in Jupyter/Brev environments)\n",
        "    import subprocess\n",
        "    import sys\n",
        "    \n",
        "    # Use system pip since venv may not have pip configured\n",
        "    result = subprocess.run(\n",
        "        [\"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\", \n",
        "         \"--index-url\", \"https://download.pytorch.org/whl/cu121\"],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n‚úì PyTorch installed successfully!\")\n",
        "        print(\"\\n‚ö†Ô∏è  IMPORTANT: Restart the kernel now:\")\n",
        "        print(\"   Click 'Kernel' ‚Üí 'Restart', then run this cell again\")\n",
        "    else:\n",
        "        # Method 2: Try with python3 -m pip\n",
        "        print(\"\\nTrying alternative installation method...\")\n",
        "        result2 = subprocess.run(\n",
        "            [\"python3\", \"-m\", \"pip\", \"install\", \"torch\", \"torchvision\", \"torchaudio\"],\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        \n",
        "        if result2.returncode == 0:\n",
        "            print(\"\\n‚úì PyTorch installed successfully!\")\n",
        "            print(\"\\n‚ö†Ô∏è  IMPORTANT: Restart the kernel now:\")\n",
        "            print(\"   Click 'Kernel' ‚Üí 'Restart', then run this cell again\")\n",
        "        else:\n",
        "            # Method 3: Direct command fallback\n",
        "            print(\"\\n‚ö†Ô∏è  Please run this command in a terminal:\")\n",
        "            print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
        "            print(\"\\nThen restart the kernel.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîç GPU VERIFICATION & SYSTEM CHECK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Verify PyTorch\n",
        "print(f\"\\n[1/3] PyTorch Status:\")\n",
        "print(f\"‚úì PyTorch {torch.__version__} is installed\")\n",
        "\n",
        "# Step 2: Verify CUDA availability\n",
        "print(\"\\n[2/3] Verifying CUDA...\")\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"‚úó ERROR: No CUDA GPU detected!\")\n",
        "    print(\"  This launchable requires an NVIDIA GPU.\")\n",
        "    print(\"  Troubleshooting:\")\n",
        "    print(\"    - Ensure you selected a GPU instance type (L40S, A10G, etc.)\")\n",
        "    print(\"    - Check NVIDIA drivers: !nvidia-smi\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"‚úì CUDA {torch.version.cuda} is available\")\n",
        "print(f\"‚úì {torch.cuda.device_count()} GPU(s) detected\")\n",
        "\n",
        "# Step 3: Check GPU specifications\n",
        "print(\"\\n[3/3] Checking GPU specifications...\")\n",
        "for i in range(torch.cuda.device_count()):\n",
        "    props = torch.cuda.get_device_properties(i)\n",
        "    gpu_memory_gb = props.total_memory / 1024**3\n",
        "    compute_cap = torch.cuda.get_device_capability(i)\n",
        "    \n",
        "    print(f\"\\n  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    print(f\"    Memory: {gpu_memory_gb:.1f} GB\")\n",
        "    print(f\"    Compute Capability: {compute_cap[0]}.{compute_cap[1]}\")\n",
        "    \n",
        "    # Validate requirements\n",
        "    issues = []\n",
        "    if compute_cap[0] < 7:\n",
        "        issues.append(f\"Compute capability {compute_cap[0]}.{compute_cap[1]} < 7.0\")\n",
        "    if gpu_memory_gb < 16:\n",
        "        issues.append(f\"Memory {gpu_memory_gb:.0f}GB < 16GB minimum\")\n",
        "    \n",
        "    if issues:\n",
        "        print(f\"    ‚ö†Ô∏è  Warnings: {'; '.join(issues)}\")\n",
        "        print(f\"    Recommended: Compute 7.0+, 24GB+ memory\")\n",
        "    else:\n",
        "        print(f\"    ‚úì Meets SGLang requirements\")\n",
        "\n",
        "# Show detailed GPU status\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìä Detailed GPU Status:\")\n",
        "print(\"=\" * 70)\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=index,name,memory.total,memory.free,temperature.gpu', '--format=csv'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True,\n",
        "        timeout=5\n",
        "    )\n",
        "    print(result.stdout)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not get detailed GPU info: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úì GPU VERIFICATION COMPLETE - Ready for SGLang!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Expected Output\n",
        "\n",
        "You should see output similar to:\n",
        "```\n",
        "======================================================================\n",
        "üîç GPU VERIFICATION & SYSTEM CHECK\n",
        "======================================================================\n",
        "\n",
        "üì¶ PyTorch version: 2.1.0+cu121\n",
        "üîß CUDA available: True\n",
        "‚úÖ CUDA version: 12.1\n",
        "üéÆ Number of GPUs: 1\n",
        "\n",
        "üöÄ GPU 0: NVIDIA A10G\n",
        "   üíæ Memory: 22.20 GB\n",
        "   ‚öôÔ∏è  Compute Capability: 8.6\n",
        "\n",
        "======================================================================\n",
        "‚úÖ GPU VERIFICATION COMPLETE\n",
        "======================================================================\n",
        "```\n",
        "\n",
        "‚úÖ **You're ready to proceed!** Your GPU meets all requirements for SGLang.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Install SGLang\n",
        "\n",
        "We'll install SGLang using the `uv` package manager for faster, more reliable installation.\n",
        "\n",
        "**Installation includes:**\n",
        "- SGLang core framework with RadixAttention\n",
        "- FlashInfer kernels (optimized CUDA attention)\n",
        "- vLLM integration for model compatibility\n",
        "- OpenAI-compatible API server\n",
        "- All necessary dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üì¶ INSTALLING SGLANG & DEPENDENCIES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Install core dependencies\n",
        "print(\"\\n[1/4] Installing core dependencies...\")\n",
        "try:\n",
        "    subprocess.run(\n",
        "        [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\", \"-q\"],\n",
        "        check=True,\n",
        "        timeout=60\n",
        "    )\n",
        "    print(\"‚úì pip, setuptools, wheel updated\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Warning: Could not upgrade pip: {e}\")\n",
        "    print(\"  Continuing with existing pip version...\")\n",
        "\n",
        "# Step 2: Install uv package manager\n",
        "print(\"\\n[2/4] Installing uv package manager...\")\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"uv\", \"-q\"], check=True, timeout=60)\n",
        "    print(\"‚úì uv installed\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error installing uv: {e}\")\n",
        "    print(\"  Falling back to pip for SGLang installation...\")\n",
        "\n",
        "# Step 3: Install SGLang\n",
        "print(\"\\n[3/4] Installing SGLang (2-3 minutes, please wait)...\")\n",
        "install_success = False\n",
        "\n",
        "# Try with uv first (faster)\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"uv\", \"pip\", \"install\", \"--system\", \"sglang[all]\", \"--prerelease=allow\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True,\n",
        "        timeout=300\n",
        "    )\n",
        "    print(\"‚úì SGLang installed via uv\")\n",
        "    install_success = True\n",
        "except Exception as e:\n",
        "    print(f\"  uv installation failed, trying pip...\")\n",
        "    \n",
        "# Fallback to pip if uv fails\n",
        "if not install_success:\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [sys.executable, \"-m\", \"pip\", \"install\", \"sglang[all]\"],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True,\n",
        "            timeout=300\n",
        "        )\n",
        "        print(\"‚úì SGLang installed via pip\")\n",
        "        install_success = True\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error installing SGLang: {e}\")\n",
        "        print(\"\\nTroubleshooting:\")\n",
        "        print(\"  - Check internet connection\")\n",
        "        print(\"  - Try: !pip install sglang[all] --verbose\")\n",
        "        sys.exit(1)\n",
        "\n",
        "# Step 4: Verify installation and install OpenAI client\n",
        "print(\"\\n[4/4] Verifying installation...\")\n",
        "try:\n",
        "    # Check SGLang\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, \"-m\", \"sglang.version\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        check=True,\n",
        "        timeout=10\n",
        "    )\n",
        "    print(f\"‚úì SGLang version: {result.stdout.strip()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not verify SGLang version: {e}\")\n",
        "\n",
        "# Install OpenAI client (needed for querying)\n",
        "try:\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"openai\", \"-q\"], check=True, timeout=30)\n",
        "    print(\"‚úì OpenAI client installed\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Warning: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úì INSTALLATION COMPLETE - Ready to launch SGLang!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Docker Installation\n",
        "\n",
        "For a pre-configured environment, you can use the official SGLang Docker image:\n",
        "\n",
        "```bash\n",
        "docker run --gpus all \\\n",
        "  --shm-size 32g \\\n",
        "  -p 30000:30000 \\\n",
        "  -v ~/.cache/huggingface:/root/.cache/huggingface \\\n",
        "  --env \"HF_TOKEN=<your_token>\" \\\n",
        "  --ipc=host \\\n",
        "  lmsysorg/sglang:latest \\\n",
        "  python3 -m sglang.launch_server --model-path meta-llama/Llama-3.1-8B-Instruct --host 0.0.0.0 --port 30000\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. HuggingFace Authentication\n",
        "\n",
        "Some models require HuggingFace authentication. You can get a token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîê HUGGINGFACE AUTHENTICATION (Optional)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Check if token is already set\n",
        "existing_token = os.environ.get('HF_TOKEN') or os.environ.get('HUGGING_FACE_HUB_TOKEN')\n",
        "\n",
        "if existing_token:\n",
        "    print(\"\\n‚úì HuggingFace token already set in environment\")\n",
        "    print(\"  Using existing token for model downloads\")\n",
        "else:\n",
        "    print(\"\\nSome models require HuggingFace authentication.\")\n",
        "    print(\"Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"\\nOptions:\")\n",
        "    print(\"  1. Enter token now\")\n",
        "    print(\"  2. Press Enter to skip (public models will work)\")\n",
        "    \n",
        "    try:\n",
        "        HF_TOKEN = input(\"\\nEnter HuggingFace token (or press Enter to skip): \").strip()\n",
        "        \n",
        "        if HF_TOKEN:\n",
        "            try:\n",
        "                from huggingface_hub import login\n",
        "                login(token=HF_TOKEN)\n",
        "                os.environ['HF_TOKEN'] = HF_TOKEN\n",
        "                print(\"\\n‚úì HuggingFace authentication successful!\")\n",
        "                print(\"  You can now access gated models\")\n",
        "            except Exception as e:\n",
        "                print(f\"\\n‚úó Authentication failed: {e}\")\n",
        "                print(\"  Continuing without authentication...\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è  Skipped authentication\")\n",
        "            print(\"  You can still use public models like Llama-3.1-8B-Instruct\")\n",
        "    except EOFError:\n",
        "        # Handle case where input() doesn't work (non-interactive)\n",
        "        print(\"\\n‚ö†Ô∏è  Non-interactive environment detected\")\n",
        "        print(\"  Continuing without authentication...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Launch Your First SGLang Server\n",
        "\n",
        "Let's launch SGLang with **Llama-3.1-8B-Instruct** - a state-of-the-art model from Meta.\n",
        "\n",
        "### What's Happening Under the Hood:\n",
        "\n",
        "1. **Model Download**: SGLang downloads the model from HuggingFace (cached for future use)\n",
        "2. **Model Loading**: Weights are loaded into GPU memory\n",
        "3. **RadixAttention Init**: Prefix tree initialized for KV cache reuse\n",
        "4. **FlashInfer Compilation**: Custom CUDA kernels compiled for your GPU\n",
        "5. **Server Ready**: OpenAI-compatible API server starts on port 30000\n",
        "\n",
        "**Expected time**: 2-5 minutes (first run), 30 seconds (subsequent runs with cached model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üöÄ LAUNCHING SGLANG SERVER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Stop any existing servers\n",
        "print(\"\\n[1/4] Checking for existing servers...\")\n",
        "try:\n",
        "    subprocess.run([\"pkill\", \"-f\", \"sglang.launch_server\"], capture_output=True, timeout=5)\n",
        "    time.sleep(2)\n",
        "    print(\"‚úì Stopped any existing SGLang servers\")\n",
        "except:\n",
        "    print(\"‚úì No existing servers found\")\n",
        "\n",
        "# Step 2: Check if port is available\n",
        "print(\"\\n[2/4] Checking port 30000...\")\n",
        "import socket\n",
        "def is_port_in_use(port):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        return s.connect_ex(('127.0.0.1', port)) == 0\n",
        "\n",
        "if is_port_in_use(30000):\n",
        "    print(\"‚úó Port 30000 is still in use!\")\n",
        "    print(\"  Attempting to free it...\")\n",
        "    subprocess.run([\"fuser\", \"-k\", \"30000/tcp\"], capture_output=True)\n",
        "    time.sleep(2)\n",
        "    if is_port_in_use(30000):\n",
        "        print(\"‚úó Could not free port 30000. Manual intervention needed:\")\n",
        "        print(\"  !lsof -ti:30000 | xargs kill -9\")\n",
        "    else:\n",
        "        print(\"‚úì Port 30000 is now available\")\n",
        "else:\n",
        "    print(\"‚úì Port 30000 is available\")\n",
        "\n",
        "# Step 3: Launch server (non-blocking)\n",
        "print(\"\\n[3/4] Starting SGLang server...\")\n",
        "print(\"  Model: meta-llama/Llama-3.1-8B-Instruct\")\n",
        "print(\"  Port: 30000\")\n",
        "print(\"  Logs: /tmp/sglang_server.log\")\n",
        "\n",
        "# Start server as background process\n",
        "server_process = subprocess.Popen(\n",
        "    [\"python3\", \"-m\", \"sglang.launch_server\",\n",
        "     \"--model-path\", \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "     \"--port\", \"30000\",\n",
        "     \"--host\", \"0.0.0.0\",\n",
        "     \"--log-level\", \"info\"],\n",
        "    stdout=open('/tmp/sglang_server.log', 'w'),\n",
        "    stderr=subprocess.STDOUT\n",
        ")\n",
        "\n",
        "print(f\"‚úì Server process started (PID: {server_process.pid})\")\n",
        "print(\"  Server is loading model in background...\")\n",
        "\n",
        "# Step 4: Wait for server to be ready\n",
        "print(\"\\n[4/4] Waiting for server to be ready (max 180s)...\")\n",
        "start_time = time.time()\n",
        "ready = False\n",
        "\n",
        "for attempt in range(90):  # 90 attempts x 2s = 180s max\n",
        "    time.sleep(2)\n",
        "    try:\n",
        "        response = requests.get(\"http://127.0.0.1:30000/health\", timeout=2)\n",
        "        if response.status_code == 200:\n",
        "            elapsed = time.time() - start_time\n",
        "            print(f\"‚úì Server is READY! (took {elapsed:.1f}s)\")\n",
        "            ready = True\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    # Show progress every 10s\n",
        "    if attempt % 5 == 0 and attempt > 0:\n",
        "        print(f\"  Still loading... ({attempt * 2}s elapsed)\")\n",
        "\n",
        "if ready:\n",
        "    # Verify server is working\n",
        "    try:\n",
        "        models_resp = requests.get(\"http://127.0.0.1:30000/v1/models\", timeout=5)\n",
        "        models = models_resp.json()\n",
        "        \n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"‚úì SERVER READY FOR INFERENCE\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"  Loaded Models: {len(models.get('data', []))}\")\n",
        "        print(f\"\\n  Endpoints:\")\n",
        "        print(f\"    - Health:      http://127.0.0.1:30000/health\")\n",
        "        print(f\"    - Models:      http://127.0.0.1:30000/v1/models\")\n",
        "        print(f\"    - Completions: http://127.0.0.1:30000/v1/completions\")\n",
        "        print(f\"    - Chat:        http://127.0.0.1:30000/v1/chat/completions\")\n",
        "        print(\"=\" * 70)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ö†Ô∏è  Server started but verification failed: {e}\")\n",
        "        print(\"  Try checking manually: !curl http://127.0.0.1:30000/health\")\n",
        "else:\n",
        "    print(\"\\n‚úó Server did not become ready in 180s\")\n",
        "    print(\"  Check logs: !tail -50 /tmp/sglang_server.log\")\n",
        "    print(\"  Check if process is running: !ps aux | grep sglang\")\n",
        "    print(\"  This may be normal for first-time model download (can take 5-10 min)\")\n",
        "    print(\"  Check status in next cell with: !curl http://127.0.0.1:30000/health\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Query the SGLang Server\n",
        "\n",
        "Now let's send requests to our running server using the SGLang Python client.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. The Magic of RadixAttention: SGLang's Secret Weapon ‚ú®\n",
        "\n",
        "Before we query the server, let's understand what makes SGLang **6x faster** than competitors.\n",
        "\n",
        "### RadixAttention: Automatic KV Cache Reuse\n",
        "\n",
        "Traditional LLM servers recompute attention for every request, even when prompts share common prefixes. **RadixAttention** solves this with:\n",
        "\n",
        "- **Radix Tree Structure**: Organizes KV caches in a tree where shared prefixes are stored once\n",
        "- **Automatic Matching**: Detects common prefixes across requests without manual hints\n",
        "- **Zero-Copy Reuse**: Instantly reuses cached attention states\n",
        "\n",
        "### Real-World Impact:\n",
        "\n",
        "| Scenario | Traditional | With RadixAttention | Speedup |\n",
        "|----------|------------|---------------------|---------|\n",
        "| Multi-turn chat | Recompute full history | Reuse conversation context | **6.4x** |\n",
        "| Few-shot prompts | Recompute examples | Reuse example embeddings | **3.2x** |\n",
        "| Shared system prompts | Recompute every time | Cache once, reuse forever | **5x** |\n",
        "| RAG with long context | Recompute documents | Cache document embeddings | **4.8x** |\n",
        "\n",
        "**Next, we'll see this in action with real benchmarks!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import requests\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üß™ TESTING SGLANG SERVER\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Pre-flight check: Ensure server is running\n",
        "print(\"\\n[Pre-flight] Checking server connectivity...\")\n",
        "try:\n",
        "    health_check = requests.get(\"http://127.0.0.1:30000/health\", timeout=5)\n",
        "    if health_check.status_code == 200:\n",
        "        print(\"‚úì Server is responding\")\n",
        "    else:\n",
        "        print(f\"‚úó Server returned unexpected status: {health_check.status_code}\")\n",
        "        print(\"  Make sure you ran the 'Launch SGLang Server' cell first!\")\n",
        "        raise SystemExit(\"Server not ready\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"‚úó Cannot connect to server on port 30000\")\n",
        "    print(\"  Make sure you ran the 'Launch SGLang Server' cell first!\")\n",
        "    print(\"  Check if server is running: !curl http://127.0.0.1:30000/health\")\n",
        "    raise SystemExit(\"Server not accessible\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Error checking server: {e}\")\n",
        "    raise SystemExit(\"Cannot verify server status\")\n",
        "\n",
        "# Configure the OpenAI client to point to our SGLang server\n",
        "print(\"\\n[Setup] Initializing OpenAI client...\")\n",
        "try:\n",
        "    import openai\n",
        "    client = openai.Client(\n",
        "        base_url=\"http://127.0.0.1:30000/v1\",\n",
        "        api_key=\"EMPTY\"  # SGLang doesn't require an API key - fully open!\n",
        "    )\n",
        "    print(\"‚úì Client initialized\")\n",
        "except ImportError:\n",
        "    print(\"‚úó OpenAI library not found. Installing...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"openai\", \"-q\"], check=True)\n",
        "    import openai\n",
        "    client = openai.Client(\n",
        "        base_url=\"http://127.0.0.1:30000/v1\",\n",
        "        api_key=\"EMPTY\"\n",
        "    )\n",
        "    print(\"‚úì OpenAI library installed and client initialized\")\n",
        "\n",
        "# Test 1: Basic Completion\n",
        "print(\"\\n[Test 1] Basic Completion\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    _ = response = client.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        prompt=\"The capital of France is\",\n",
        "        max_tokens=50,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Success!\")\n",
        "    print(f\"   Prompt: \\\"The capital of France is\\\"\")\n",
        "    print(f\"   Response: \\\"{response.choices[0].text.strip()}\\\"\")\n",
        "    print(f\"   Tokens: {response.usage.total_tokens}\")\n",
        "    print(f\"   Latency: {latency:.3f}s\")\n",
        "    print(f\"   Throughput: {response.usage.total_tokens / latency:.1f} tokens/s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Test 2: Chat Completion\n",
        "print(\"\\n[Test 2] Chat Completion\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    _ = chat_response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in explaining complex technology simply.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain what SGLang is and why it's revolutionary in 2 sentences.\"}\n",
        "        ],\n",
        "        max_tokens=150,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Success!\")\n",
        "    print(f\"   User: \\\"Explain what SGLang is and why it's revolutionary\\\"\")\n",
        "    print(f\"   Assistant: \\\"{chat_response.choices[0].message.content}\\\"\")\n",
        "    print(f\"   Tokens: {chat_response.usage.total_tokens}\")\n",
        "    print(f\"   Latency: {latency:.3f}s\")\n",
        "    print(f\"   Throughput: {chat_response.usage.total_tokens / latency:.1f} tokens/s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Test 3: Structured Output\n",
        "print(\"\\n[Test 3] Structured Generation (JSON)\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    _ = structured_response = client.chat.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an API that returns JSON only. No other text.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Generate a JSON object with 3 AI companies and their main products.\"}\n",
        "        ],\n",
        "        max_tokens=200,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚úÖ Success!\")\n",
        "    print(f\"   Response: {structured_response.choices[0].message.content}\")\n",
        "    print(f\"   Latency: {latency:.3f}s\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ ALL TESTS PASSED!\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RadixAttention in Action: See the Speedup! üöÄ\n",
        "\n",
        "Let's demonstrate the power of RadixAttention with a real benchmark. We'll send multiple requests with shared prefixes and measure the speedup.\n",
        "\n",
        "### The Test:\n",
        "- **10 requests** with the same long system prompt\n",
        "- **Without RadixAttention**: Each request recomputes the entire prompt\n",
        "- **With RadixAttention** (SGLang): First request computes, rest reuse cached KV\n",
        "\n",
        "This is exactly what happens in production: chatbots, RAG systems, and APIs all reuse system prompts!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import statistics\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"‚ö° RADIXATTENTION BENCHMARK\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Create a long system prompt (this simulates a RAG context or few-shot examples)\n",
        "system_prompt = \"\"\"You are an expert AI coding assistant. You follow these principles:\n",
        "1. Write clean, readable, and maintainable code\n",
        "2. Use proper error handling and input validation\n",
        "3. Follow PEP 8 style guidelines for Python\n",
        "4. Add helpful comments and docstrings\n",
        "5. Consider edge cases and potential bugs\n",
        "6. Optimize for clarity over cleverness\n",
        "7. Use type hints when helpful\n",
        "8. Write testable code with clear interfaces\n",
        "9. Follow SOLID principles\n",
        "10. Consider security implications\n",
        "\n",
        "You have expertise in: Python, JavaScript, Go, Rust, C++, Java, TypeScript,\n",
        "React, Node.js, Django, FastAPI, PostgreSQL, MongoDB, Redis, Docker, Kubernetes,\n",
        "AWS, GCP, Azure, CI/CD, Testing, Security, Performance Optimization, and more.\"\"\"\n",
        "\n",
        "# Questions to ask\n",
        "questions = [\n",
        "    \"Write a Python function to find prime numbers\",\n",
        "    \"Create a binary search implementation\",\n",
        "    \"Show me a quicksort algorithm\",\n",
        "    \"Write a function to reverse a linked list\",\n",
        "    \"Implement a hash table in Python\",\n",
        "    \"Create a depth-first search function\",\n",
        "    \"Write a function to detect cycles in a graph\",\n",
        "    \"Implement a min heap data structure\",\n",
        "    \"Show me how to do merge sort\",\n",
        "    \"Write a function to check if a string is a palindrome\",\n",
        "]\n",
        "\n",
        "print(f\"\\nüìä Test Configuration:\")\n",
        "print(f\"   System Prompt Length: {len(system_prompt)} characters (~{len(system_prompt.split())} tokens)\")\n",
        "print(f\"   Number of Requests: {len(questions)}\")\n",
        "print(f\"   Expected Behavior:\")\n",
        "print(f\"      - Request 1: Full computation (slower)\")\n",
        "print(f\"      - Requests 2-10: RadixAttention reuse (MUCH faster)\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Running benchmark...\\n\")\n",
        "\n",
        "latencies = []\n",
        "first_request_time = None\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    start = time.time()\n",
        "    \n",
        "    try:\n",
        "        _ = response = client.chat.completions.create(\n",
        "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": question}\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        \n",
        "        latency = time.time() - start\n",
        "        latencies.append(latency)\n",
        "        \n",
        "        if i == 1:\n",
        "            first_request_time = latency\n",
        "        \n",
        "        # Show progress\n",
        "        speedup = first_request_time / latency if i > 1 else 1.0\n",
        "        emoji = \"üî•\" if speedup > 1.5 else \"‚úÖ\"\n",
        "        print(f\"   {emoji} Request {i:2d}: {latency:.3f}s (speedup: {speedup:.2f}x)\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Request {i} failed: {e}\")\n",
        "\n",
        "# Calculate statistics\n",
        "if len(latencies) >= 2:\n",
        "    first_req = latencies[0]\n",
        "    subsequent_reqs = latencies[1:]\n",
        "    \n",
        "    avg_subsequent = statistics.mean(subsequent_reqs)\n",
        "    speedup = first_req / avg_subsequent\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"üìà BENCHMARK RESULTS\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n‚è±Ô∏è  Latency:\")\n",
        "    print(f\"   First Request (cold):        {first_req:.3f}s\")\n",
        "    print(f\"   Average Subsequent (cached): {avg_subsequent:.3f}s\")\n",
        "    print(f\"   Best Subsequent:             {min(subsequent_reqs):.3f}s\")\n",
        "    print(f\"   Worst Subsequent:            {max(subsequent_reqs):.3f}s\")\n",
        "    \n",
        "    print(f\"\\nüöÄ RadixAttention Speedup:\")\n",
        "    print(f\"   Average: {speedup:.2f}x faster\")\n",
        "    print(f\"   Best:    {first_req / min(subsequent_reqs):.2f}x faster\")\n",
        "    \n",
        "    print(f\"\\nüí∞ Cost Savings:\")\n",
        "    total_saved = (first_req - avg_subsequent) * len(subsequent_reqs)\n",
        "    print(f\"   Time saved: {total_saved:.2f}s across {len(subsequent_reqs)} requests\")\n",
        "    print(f\"   In production (1M requests/day): ~{(total_saved * 1000000 / len(subsequent_reqs) / 3600):.1f} hours saved!\")\n",
        "    \n",
        "    print(f\"\\nüéØ Key Insight:\")\n",
        "    print(f\"   The system prompt ({len(system_prompt.split())} tokens) was cached after\")\n",
        "    print(f\"   the first request and reused instantly for all subsequent requests.\")\n",
        "    print(f\"   This is the power of RadixAttention! üî•\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Not enough successful requests to calculate statistics\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Production Optimization: FP8 Quantization ‚öôÔ∏è\n",
        "\n",
        "Now let's level up with **FP8 quantization** - reduce memory by 2x and increase speed by 1.5x with minimal quality loss.\n",
        "\n",
        "### What is FP8 Quantization?\n",
        "\n",
        "- **FP8 (8-bit Floating Point)**: Uses 8-bit instead of 16-bit weights\n",
        "- **Memory**: Reduces VRAM usage by ~50% (fit larger models or bigger batches)\n",
        "- **Speed**: Faster computation with Tensor Cores on modern NVIDIA GPUs (A100, H100, L4)\n",
        "- **Quality**: <1% degradation on most tasks\n",
        "\n",
        "### When to Use FP8:\n",
        "\n",
        "‚úÖ Production deployments (save costs)  \n",
        "‚úÖ Large batch sizes (2x throughput)  \n",
        "‚úÖ Bigger models on smaller GPUs  \n",
        "‚úÖ Lower latency requirements\n",
        "\n",
        "Let's restart the server with FP8 enabled:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop previous server (if running)\n",
        "import requests\n",
        "\n",
        "try:\n",
        "    requests.post(\"http://127.0.0.1:30000/shutdown\", timeout=5)\n",
        "    time.sleep(5)\n",
        "    print(\"Previous server stopped.\")\n",
        "except:\n",
        "    print(\"No previous server to stop.\")\n",
        "\n",
        "# Launch with FP8 quantization for better performance\n",
        "def run_quantized_server():\n",
        "    \"\"\"Run SGLang server with FP8 quantization\"\"\"\n",
        "    global server_process\n",
        "    \n",
        "    cmd = [\n",
        "        \"python3\", \"-m\", \"sglang.launch_server\",\n",
        "        \"--model-path\", \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"--port\", \"30000\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--dtype\", \"bfloat16\",\n",
        "        \"--quantization\", \"fp8\",  # Enable FP8 quantization\n",
        "        \"--mem-fraction-static\", \"0.85\"  # Use 85% of GPU memory for KV cache\n",
        "    ]\n",
        "    \n",
        "    server_process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    for line in server_process.stdout:\n",
        "        print(line, end='')\n",
        "    \n",
        "    return server_process\n",
        "\n",
        "print(\"Launching SGLang server with FP8 quantization...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "quantized_server_thread = threading.Thread(target=run_quantized_server, daemon=True)\n",
        "quantized_server_thread.start()\n",
        "\n",
        "time.sleep(30)\n",
        "print(\"\\nQuantized server ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Benchmark Server Performance\n",
        "\n",
        "Let's benchmark the server's throughput and latency under concurrent load.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmark the server\n",
        "import concurrent.futures\n",
        "import time\n",
        "import statistics\n",
        "\n",
        "def send_request(prompt, max_tokens=256):\n",
        "    \"\"\"Send a single request and measure latency\"\"\"\n",
        "    start_time = time.time()\n",
        "    \n",
        "    response = client.completions.create(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    \n",
        "    latency = time.time() - start_time\n",
        "    tokens = response.usage.total_tokens\n",
        "    \n",
        "    return {\n",
        "        'latency': latency,\n",
        "        'tokens': tokens,\n",
        "        'tokens_per_second': tokens / latency\n",
        "    }\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"Write a short story about a robot learning to paint.\",\n",
        "    \"Explain quantum computing to a 5-year-old.\",\n",
        "    \"What are the key features of Python programming?\",\n",
        "    \"Describe the process of photosynthesis.\",\n",
        "    \"What is the history of artificial intelligence?\",\n",
        "]\n",
        "\n",
        "print(\"Running benchmark with 5 concurrent requests...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Send concurrent requests\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    futures = [executor.submit(send_request, prompt) for prompt in test_prompts]\n",
        "    results = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Calculate statistics\n",
        "latencies = [r['latency'] for r in results]\n",
        "tokens_per_sec = [r['tokens_per_second'] for r in results]\n",
        "total_tokens = sum(r['tokens'] for r in results)\n",
        "\n",
        "print(f\"\\nBenchmark Results:\")\n",
        "print(f\"  Total time: {total_time:.2f}s\")\n",
        "print(f\"  Total tokens: {total_tokens}\")\n",
        "print(f\"  Average latency: {statistics.mean(latencies):.2f}s\")\n",
        "print(f\"  P50 latency: {statistics.median(latencies):.2f}s\")\n",
        "print(f\"  P95 latency: {sorted(latencies)[int(len(latencies)*0.95)]:.2f}s\")\n",
        "print(f\"  Average throughput: {statistics.mean(tokens_per_sec):.2f} tokens/s\")\n",
        "print(f\"  Total throughput: {total_tokens/total_time:.2f} tokens/s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Serving Multi-Modal Models (Vision + Language)\n",
        "\n",
        "SGLang supports vision-language models like LLaVA. Let's try serving a multi-modal model.\n",
        "\n",
        "**Note**: This requires more GPU memory. Skip this section if you have < 24GB VRAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch SGLang with LLaVA model (Vision-Language)\n",
        "# Stop previous server first\n",
        "try:\n",
        "    requests.post(\"http://127.0.0.1:30000/shutdown\", timeout=5)\n",
        "    time.sleep(5)\n",
        "    print(\"Previous server stopped.\")\n",
        "except:\n",
        "    print(\"No previous server to stop.\")\n",
        "\n",
        "def run_multimodal_server():\n",
        "    \"\"\"Run SGLang server with LLaVA (vision-language model)\"\"\"\n",
        "    global server_process\n",
        "    \n",
        "    cmd = [\n",
        "        \"python3\", \"-m\", \"sglang.launch_server\",\n",
        "        \"--model-path\", \"lmms-lab/llama3-llava-next-8b\",\n",
        "        \"--port\", \"30000\",\n",
        "        \"--host\", \"0.0.0.0\",\n",
        "        \"--tp-size\", \"1\",  # Tensor parallelism size\n",
        "        \"--chat-template\", \"llava_llama_3\"\n",
        "    ]\n",
        "    \n",
        "    server_process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    for line in server_process.stdout:\n",
        "        print(line, end='')\n",
        "    \n",
        "    return server_process\n",
        "\n",
        "print(\"Launching SGLang server with LLaVA multi-modal model...\")\n",
        "print(\"This may take longer as it's a larger model...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "multimodal_thread = threading.Thread(target=run_multimodal_server, daemon=True)\n",
        "multimodal_thread.start()\n",
        "\n",
        "time.sleep(45)\n",
        "print(\"\\nMulti-modal server ready! You can now query it with images.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Using the HTTP API Directly\n",
        "\n",
        "SGLang exposes an OpenAI-compatible HTTP API. You can query it with curl or any HTTP client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query the server using HTTP API directly\n",
        "import requests\n",
        "import json\n",
        "\n",
        "# Health check\n",
        "_ = health = requests.get(\"http://127.0.0.1:30000/health\")\n",
        "print(f\"Server health: {health.json()}\")\n",
        "\n",
        "# Get model info\n",
        "_ = model_info = requests.get(\"http://127.0.0.1:30000/v1/models\")\n",
        "print(f\"\\nAvailable models: {json.dumps(model_info.json(), indent=2)}\")\n",
        "\n",
        "# Send a completion request via HTTP\n",
        "payload = {\n",
        "    \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"prompt\": \"What are the benefits of using SGLang?\",\n",
        "    \"max_tokens\": 200,\n",
        "    \"temperature\": 0.7,\n",
        "    \"stream\": False\n",
        "}\n",
        "\n",
        "_ = response = requests.post(\n",
        "    \"http://127.0.0.1:30000/v1/completions\",\n",
        "    json=payload,\n",
        "    headers={\"Content-Type\": \"application/json\"}\n",
        ")\n",
        "\n",
        "result = response.json()\n",
        "print(f\"\\nHTTP API Response:\")\n",
        "print(f\"Prompt: {payload['prompt']}\")\n",
        "print(f\"Completion: {result['choices'][0]['text']}\")\n",
        "print(f\"Tokens used: {result['usage']['total_tokens']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Best Practices & Tips\n",
        "\n",
        "### Memory Optimization\n",
        "- Use `--mem-fraction-static` to control KV cache size (default 0.9)\n",
        "- Enable quantization: `--quantization fp8` or `--quantization awq`\n",
        "- Use smaller context lengths for higher throughput\n",
        "\n",
        "### Performance Tuning\n",
        "- Adjust batch size with `--max-running-requests` (default 4096)\n",
        "- Use tensor parallelism for large models: `--tp-size 2` or `--tp-size 4`\n",
        "- Enable continuous batching for better throughput\n",
        "\n",
        "### Common Issues\n",
        "1. **CUDA Out of Memory**: Reduce `--mem-fraction-static` or use quantization\n",
        "2. **Slow First Request**: Model loading takes time; subsequent requests are fast\n",
        "3. **Port Already in Use**: Change port with `--port 30001` or kill existing process\n",
        "\n",
        "### Production Deployment\n",
        "- Use Docker for reproducible deployments\n",
        "- Set up monitoring with Prometheus/Grafana\n",
        "- Use a load balancer for multiple instances\n",
        "- Enable logging: `--log-level info`\n",
        "\n",
        "### Useful Commands\n",
        "```bash\n",
        "# Check SGLang version\n",
        "python -m sglang.version\n",
        "\n",
        "# List available models\n",
        "python -m sglang.list_models\n",
        "\n",
        "# Run benchmarks\n",
        "python -m sglang.bench_serving \\\n",
        "  --backend sglang \\\n",
        "  --model meta-llama/Llama-3.1-8B-Instruct \\\n",
        "  --port 30000 \\\n",
        "  --dataset-name random \\\n",
        "  --random-input 512 \\\n",
        "  --random-output 256 \\\n",
        "  --max-concurrency 16\n",
        "```\n",
        "\n",
        "### Resources\n",
        "- [SGLang GitHub](https://github.com/sgl-project/sglang)\n",
        "- [SGLang Documentation](https://docs.sglang.ai/)\n",
        "- [FlashInfer Kernels](https://github.com/flashinfer-ai/flashinfer)\n",
        "- [Model Compatibility List](https://docs.sglang.ai/backend/supported_models.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Advanced: Streaming Responses\n",
        "\n",
        "SGLang supports streaming responses for real-time output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of streaming responses\n",
        "print(\"Testing streaming response...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "stream = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Write a haiku about GPU computing.\"}\n",
        "    ],\n",
        "    max_tokens=100,\n",
        "    temperature=0.8,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "print(\"\\nStreaming response:\")\n",
        "for chunk in stream:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end='', flush=True)\n",
        "\n",
        "print(\"\\n\\n\" + \"=\"*60)\n",
        "print(\"Streaming complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Debugging Tools\n",
        "\n",
        "If something isn't working, use this cell to diagnose issues:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import requests\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üîß SYSTEM DIAGNOSTICS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# 1. GPU Status\n",
        "print(\"\\n[1] GPU Memory Status:\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        ['nvidia-smi', '--query-gpu=index,name,memory.used,memory.total', '--format=csv,noheader'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=5\n",
        "    )\n",
        "    print(result.stdout)\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Could not get GPU status: {e}\")\n",
        "\n",
        "# 2. Running SGLang Processes\n",
        "print(\"\\n[2] SGLang Processes:\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"ps\", \"aux\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=5\n",
        "    )\n",
        "    sglang_procs = [line for line in result.stdout.split('\\n') if 'sglang.launch_server' in line]\n",
        "    if sglang_procs:\n",
        "        print(\"  Found {} SGLang process(es):\".format(len(sglang_procs)))\n",
        "        for proc in sglang_procs:\n",
        "            print(f\"  {proc}\")\n",
        "    else:\n",
        "        print(\"  ‚úó No SGLang processes found\")\n",
        "except Exception as e:\n",
        "    print(f\"‚úó Could not list processes: {e}\")\n",
        "\n",
        "# 3. Port Status\n",
        "print(\"\\n[3] Port 30000 Status:\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"lsof\", \"-i\", \":30000\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=5\n",
        "    )\n",
        "    if result.stdout:\n",
        "        print(f\"  Port 30000 is IN USE:\")\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"  Port 30000 is FREE\")\n",
        "except Exception as e:\n",
        "    print(f\"  Port 30000 appears free (or lsof not available)\")\n",
        "\n",
        "# 4. Server Health Check\n",
        "print(\"\\n[4] Server Health Check:\")\n",
        "try:\n",
        "    response = requests.get(\"http://127.0.0.1:30000/health\", timeout=3)\n",
        "    print(f\"  ‚úì Server is responding: {response.json()}\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"  ‚úó Cannot connect to server on port 30000\")\n",
        "except Exception as e:\n",
        "    print(f\"  ‚úó Health check failed: {e}\")\n",
        "\n",
        "# 5. Server Logs (last 20 lines)\n",
        "print(\"\\n[5] Server Logs (last 20 lines):\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"tail\", \"-20\", \"/tmp/sglang_server.log\"],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=5\n",
        "    )\n",
        "    if result.stdout:\n",
        "        print(result.stdout)\n",
        "    else:\n",
        "        print(\"  No logs found at /tmp/sglang_server.log\")\n",
        "except Exception as e:\n",
        "    print(f\"  Could not read logs: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úì Diagnostics complete\")\n",
        "print(\"=\" * 70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Cleanup\n",
        "\n",
        "Let's stop the server and clean up resources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"üßπ CLEANUP - STOPPING ALL SGLANG SERVERS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Step 1: Try graceful shutdown via API\n",
        "print(\"\\n[1/3] Attempting graceful shutdown...\")\n",
        "try:\n",
        "    import requests\n",
        "    response = requests.post(\"http://127.0.0.1:30000/shutdown\", timeout=5)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úì Server acknowledged shutdown request\")\n",
        "        time.sleep(3)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Server returned status {response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not reach server API: {e}\")\n",
        "\n",
        "# Step 2: Kill all sglang processes\n",
        "print(\"\\n[2/3] Stopping all SGLang processes...\")\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\"pkill\", \"-9\", \"-f\", \"sglang.launch_server\"],\n",
        "        capture_output=True,\n",
        "        timeout=5\n",
        "    )\n",
        "    time.sleep(2)\n",
        "    \n",
        "    # Check if any processes remain\n",
        "    check = subprocess.run(\n",
        "        [\"pgrep\", \"-f\", \"sglang.launch_server\"],\n",
        "        capture_output=True\n",
        "    )\n",
        "    if check.returncode == 0:\n",
        "        print(\"‚ö†Ô∏è  Some processes may still be running\")\n",
        "        print(\"  Try manually: !pkill -9 -f sglang.launch_server\")\n",
        "    else:\n",
        "        print(\"‚úì All SGLang processes stopped\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not kill processes: {e}\")\n",
        "\n",
        "# Step 3: Clean up temp files\n",
        "print(\"\\n[3/3] Cleaning up temporary files...\")\n",
        "try:\n",
        "    import os\n",
        "    if os.path.exists('/tmp/sglang_server.log'):\n",
        "        log_size = os.path.getsize('/tmp/sglang_server.log')\n",
        "        print(f\"‚úì Server logs available: /tmp/sglang_server.log ({log_size} bytes)\")\n",
        "        print(\"  Run to view: !tail -50 /tmp/sglang_server.log\")\n",
        "    else:\n",
        "        print(\"  No log files found\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è  Could not check temp files: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ CLEANUP COMPLETE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüéâ Tutorial complete! You now know how to:\")\n",
        "print(\"  ‚úì Install and configure SGLang on NVIDIA GPUs\")\n",
        "print(\"  ‚úì Launch servers with proper process management\")\n",
        "print(\"  ‚úì Query servers using Python and HTTP APIs\")\n",
        "print(\"  ‚úì Leverage RadixAttention for massive speedups\")\n",
        "print(\"  ‚úì Benchmark and optimize for production\")\n",
        "print(\"  ‚úì Debug issues with diagnostic tools\")\n",
        "\n",
        "print(\"\\nüìö Next steps:\")\n",
        "print(\"  - Try different models from HuggingFace\")\n",
        "print(\"  - Experiment with FP8 quantization\")\n",
        "print(\"  - Test multi-modal models (LLaVA)\")\n",
        "print(\"  - Deploy to production with Docker\")\n",
        "print(\"  - Scale with tensor parallelism on multi-GPU setups\")\n",
        "\n",
        "print(\"\\nüí° Resources:\")\n",
        "print(\"  - SGLang Docs: https://docs.sglang.ai/\")\n",
        "print(\"  - GitHub: https://github.com/sgl-project/sglang\")\n",
        "print(\"  - Discord: https://discord.gg/NVDyv7TUgJ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Need Help?\n",
        "\n",
        "Join the community:\n",
        "- [SGLang GitHub Discussions](https://github.com/sgl-project/sglang/discussions)\n",
        "- [NVIDIA Brev Discord](https://discord.gg/NVDyv7TUgJ)\n",
        "- [SGLang Documentation](https://docs.sglang.ai/)\n",
        "\n",
        "Built with üíö by the NVIDIA Brev team\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

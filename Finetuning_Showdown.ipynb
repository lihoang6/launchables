{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Banner Image -->\n",
    "<img src=\"https://uohmivykqgnnbiouffke.supabase.co/storage/v1/object/public/landingpage/brevdevnotebooks.png\" width=\"100%\">\n",
    "\n",
    "<!-- Links -->\n",
    "<center>\n",
    "  <a href=\"https://console.brev.dev\" style=\"color: #06b6d4;\">Console</a> ‚Ä¢\n",
    "  <a href=\"https://brev.dev\" style=\"color: #06b6d4;\">Docs</a> ‚Ä¢\n",
    "  <a href=\"/\" style=\"color: #06b6d4;\">Templates</a> ‚Ä¢\n",
    "  <a href=\"https://discord.gg/NVDyv7TUgJ\" style=\"color: #06b6d4;\">Discord</a>\n",
    "</center>\n",
    "\n",
    "# ‚ö° Fine-Tuning Showdown: Unsloth vs HuggingFace\n",
    "\n",
    "## Real Benchmarks. Same Model. Same Dataset. YOUR GPU.\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); padding: 30px; border-radius: 10px; color: white; margin-bottom: 20px;\">\n",
    "  <h2 style=\"margin-top: 0; color: white;\">üéØ What You'll Discover</h2>\n",
    "  <p style=\"font-size: 18px; line-height: 1.6;\">\n",
    "    <strong>Stop guessing. Start measuring.</strong><br/><br/>\n",
    "    We'll train <strong>the same model</strong> with <strong>the same dataset</strong> using two frameworks:<br/>\n",
    "    ‚ö° <strong>Unsloth</strong> - Claims 2√ó speed, let's verify<br/>\n",
    "    ü§ó <strong>HuggingFace</strong> - Vanilla baseline (PEFT + Trainer)<br/>\n",
    "    <br/>\n",
    "    <strong>üî• GPU starts training in 60 seconds. Side-by-side results in 10 minutes.</strong>\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- **GPU**: NVIDIA GPU with 16GB+ VRAM (A100, H100, L40S, RTX 4090, RTX 3090)\n",
    "- **CUDA**: 11.8+ or 12.1+\n",
    "- **Python**: 3.10+\n",
    "- **Disk Space**: 20GB free (for models + datasets)\n",
    "\n",
    "---\n",
    "\n",
    "## üé¨ Quick Start: What Happens Next\n",
    "\n",
    "The next few cells will:\n",
    "1. **Install Unsloth** (~30 sec)\n",
    "2. **Load Qwen 1.5B model** (~15 sec)\n",
    "3. **Start training** immediately (2-3 min)\n",
    "4. **Train with vanilla HF** (same config)\n",
    "5. **Compare ALL metrics** side-by-side\n",
    "\n",
    "**Total time: ~10 minutes for complete comparison** ‚ö°\n",
    "\n",
    "---\n",
    "\n",
    "#### üí¨ Questions? Join us on [Discord](https://discord.gg/NVDyv7TUgJ) or reach out on [X/Twitter](https://x.com/brevdev)\n",
    "\n",
    "**üìù Notebook Tips**: Press `Shift + Enter` to run cells. A `*` means running, a number means complete.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify GPU Setup üéÆ\n",
    "\n",
    "Let's make sure your NVIDIA GPU is ready for fine-tuning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: GPU Verification\n",
    "# =========================\n",
    "# Quick check that GPU is available and has enough memory\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üéÆ GPU STATUS CHECK\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check nvidia-smi\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"nvidia-smi\", \"--query-gpu=name,memory.total,driver_version\", \"--format=csv,noheader\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        gpu_info = result.stdout.strip().split(\", \")\n",
    "        print(f\"‚úÖ GPU Detected: {gpu_info[0]}\")\n",
    "        print(f\"‚úÖ VRAM: {gpu_info[1]}\")\n",
    "        print(f\"‚úÖ Driver: {gpu_info[2]}\")\n",
    "        \n",
    "        # Check if enough memory\n",
    "        vram_str = gpu_info[1].replace(' MiB', '').strip()\n",
    "        try:\n",
    "            vram_gb = float(vram_str) / 1024\n",
    "            if vram_gb < 16:\n",
    "                print(f\"\\n‚ö†Ô∏è  Warning: Your GPU has {vram_gb:.1f}GB VRAM.\")\n",
    "                print(\"   This notebook recommends 16GB+ for full comparison.\")\n",
    "                print(\"   Training may still work with smaller models or reduced batch sizes.\")\n",
    "        except:\n",
    "            print(\"   (Could not parse VRAM size)\")\n",
    "    else:\n",
    "        print(\"‚ùå nvidia-smi failed. Is NVIDIA driver installed?\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå nvidia-smi not found. Please install NVIDIA drivers.\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error checking GPU: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check PyTorch CUDA\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Checking PyTorch CUDA support...\\n\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ PyTorch {torch.__version__}\")\n",
    "        print(f\"‚úÖ CUDA {torch.version.cuda}\")\n",
    "        print(f\"‚úÖ {torch.cuda.device_count()} GPU(s) available\")\n",
    "    else:\n",
    "        print(\"‚ùå PyTorch installed but CUDA not available\")\n",
    "        sys.exit(1)\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  PyTorch not found. Installing...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"torch\", \"-q\"], check=True)\n",
    "    import torch\n",
    "    print(f\"‚úÖ PyTorch {torch.__version__} installed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ GPU READY FOR TRAINING!\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Unsloth (Fastest Method) ‚ö°\n",
    "\n",
    "Unsloth claims 2√ó faster training with 60% less memory. Let's verify!\n",
    "\n",
    "We'll train with Unsloth first, then compare with vanilla HuggingFace.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install Unsloth & Dependencies\n",
    "# =======================================\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö° INSTALLING UNSLOTH & DEPENDENCIES\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "install_start = time.time()\n",
    "\n",
    "packages = [\n",
    "    \"unsloth\",\n",
    "    \"transformers\",\n",
    "    \"datasets\",\n",
    "    \"peft\",\n",
    "    \"trl\",\n",
    "    \"accelerate\",\n",
    "    \"bitsandbytes\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"pandas\"\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing packages (this may take 1-2 minutes)...\\n\")\n",
    "\n",
    "for package in packages:\n",
    "    print(f\"   Installing {package}... \", end=\"\", flush=True)\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"],\n",
    "            capture_output=True,\n",
    "            timeout=180,\n",
    "            check=False\n",
    "        )\n",
    "        print(\"‚úÖ\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è \")\n",
    "\n",
    "install_time = time.time() - install_start\n",
    "\n",
    "print(f\"\\n‚úÖ Installation complete in {install_time:.1f}s\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model & Dataset üì¶\n",
    "\n",
    "**Model:** Qwen2.5-1.5B-Instruct (fast to train, production-quality)  \n",
    "**Dataset:** OpenHermes-2.5 (5K samples, high-quality instruction data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Model + Dataset\n",
    "# =============================\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üì¶ LOADING MODEL & DATASET\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "MODEL_NAME = \"unsloth/Qwen2.5-1.5B-Instruct\"\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "print(f\"[1/2] Loading model: {MODEL_NAME}...\\n\")\n",
    "model_start = time.time()\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    model_time = time.time() - model_start\n",
    "    print(f\"   ‚úÖ Model loaded in {model_time:.1f}s\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        print(f\"   üìä GPU Memory: {memory_allocated:.2f} GB\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Model loading failed: {e}\")\n",
    "    raise\n",
    "\n",
    "print(f\"[2/2] Loading dataset: OpenHermes-2.5 (5,000 samples)...\\n\")\n",
    "dataset_start = time.time()\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\"teknium/OpenHermes-2.5\", split=\"train[:5000]\")\n",
    "    dataset_time = time.time() - dataset_start\n",
    "    print(f\"   ‚úÖ Dataset loaded in {dataset_time:.1f}s\")\n",
    "    print(f\"   üìù {len(dataset)} training samples\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è  Primary dataset failed, using backup...\")\n",
    "    dataset = load_dataset(\"mlabonne/guanaco-llama2-1k\", split=\"train\")\n",
    "    print(f\"   ‚úÖ Backup dataset: {len(dataset)} samples\\n\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚úÖ READY TO START TRAINING!\")\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üî• START TRAINING: Method 1 - Unsloth\n",
    "\n",
    "### GPU ACTIVE NOW!\n",
    "\n",
    "**Configuration** (identical for both methods):\n",
    "- LoRA rank: 16, alpha: 32\n",
    "- Batch size: 2 √ó 4 = 8 effective\n",
    "- Learning rate: 2e-4, Steps: 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Train with Unsloth\n",
    "# ===========================\n",
    "\n",
    "from unsloth import is_bfloat16_supported\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"‚ö° METHOD 1: UNSLOTH\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüî• GPU TRAINING STARTING...\\n\")\n",
    "\n",
    "# Configure LoRA\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA configured\\n\")\n",
    "\n",
    "# Format dataset\n",
    "def format_prompts(examples):\n",
    "    texts = []\n",
    "    for convs in examples[\"conversations\"]:\n",
    "        try:\n",
    "            text = tokenizer.apply_chat_template(convs, tokenize=False, add_generation_prompt=False)\n",
    "            texts.append(text)\n",
    "        except:\n",
    "            texts.append(str(convs))\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(format_prompts, batched=True)\n",
    "print(\"‚úÖ Dataset formatted\\n\")\n",
    "\n",
    "# Reset GPU stats\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "training_start = time.time()\n",
    "\n",
    "# Training!\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs_unsloth\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Collect metrics\n",
    "unsloth_time = time.time() - training_start\n",
    "unsloth_memory = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "unsloth_loss = trainer_stats.training_loss\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ UNSLOTH TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚è±Ô∏è  Time: {unsloth_time/60:.2f} min ({unsloth_time:.1f}s)\")\n",
    "print(f\"üíæ Peak Memory: {unsloth_memory:.2f} GB\")\n",
    "print(f\"üìâ Final Loss: {unsloth_loss:.4f}\")\n",
    "\n",
    "# Save checkpoint\n",
    "model.save_pretrained(\"unsloth_lora_model\")\n",
    "tokenizer.save_pretrained(\"unsloth_lora_model\")\n",
    "print(f\"üíæ Checkpoint saved\\n\")\n",
    "\n",
    "# Store results\n",
    "unsloth_results = {\n",
    "    \"method\": \"Unsloth\",\n",
    "    \"time_seconds\": unsloth_time,\n",
    "    \"memory_gb\": unsloth_memory,\n",
    "    \"loss\": unsloth_loss,\n",
    "}\n",
    "\n",
    "print(\"=\"*80 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
